# Prometheus Alerting Rules for Unified System
# Phase 3 - Production Monitoring

groups:
  - name: agent_health
    interval: 30s
    rules:
      # Essential agent down
      - alert: EssentialAgentDown
        expr: up{job="unified_agents", required="true"} == 0
        for: 1m
        labels:
          severity: critical
          component: agent
        annotations:
          summary: "Essential agent {{ $labels.agent_name }} is down"
          description: "Essential agent {{ $labels.agent_name }} has been down for more than 1 minute"
          
      # Optional agent repeatedly failing
      - alert: OptionalAgentFailureLoop
        expr: rate(agent_start_failures_total[5m]) > 0.5
        for: 5m
        labels:
          severity: warning
          component: agent
        annotations:
          summary: "Agent {{ $labels.agent_name }} is in failure loop"
          description: "Agent {{ $labels.agent_name }} has failed to start {{ $value }} times per second over 5 minutes"
          
      # High agent restart rate
      - alert: HighAgentRestartRate
        expr: rate(agent_restarts_total[10m]) > 0.1
        for: 10m
        labels:
          severity: warning
          component: stability
        annotations:
          summary: "High restart rate detected"
          description: "Agents are restarting at {{ $value }} per second"

  - name: resource_usage
    interval: 30s
    rules:
      # High memory usage
      - alert: HighMemoryUsage
        expr: (process_resident_memory_bytes / 1024 / 1024) > 7000
        for: 5m
        labels:
          severity: warning
          component: resources
        annotations:
          summary: "High memory usage by {{ $labels.agent_name }}"
          description: "Agent {{ $labels.agent_name }} is using {{ $value }}MB of memory"
          
      # VRAM exhaustion
      - alert: VRAMExhaustion
        expr: gpu_memory_available_mb < 500
        for: 2m
        labels:
          severity: critical
          component: gpu
        annotations:
          summary: "GPU VRAM critically low"
          description: "Only {{ $value }}MB of VRAM available"
          
      # CPU throttling
      - alert: CPUThrottling
        expr: rate(process_cpu_seconds_total[5m]) > 0.8
        for: 5m
        labels:
          severity: warning
          component: cpu
        annotations:
          summary: "Agent {{ $labels.agent_name }} consuming high CPU"
          description: "Agent using {{ $value | humanizePercentage }} of CPU"

  - name: system_performance
    interval: 30s
    rules:
      # Slow health checks
      - alert: SlowHealthChecks
        expr: health_check_duration_seconds > 5
        for: 5m
        labels:
          severity: warning
          component: health
        annotations:
          summary: "Health checks are slow"
          description: "Health check for {{ $labels.agent_name }} taking {{ $value }}s"
          
      # High latency
      - alert: HighRequestLatency
        expr: histogram_quantile(0.95, rate(request_duration_seconds_bucket[5m])) > 2
        for: 5m
        labels:
          severity: warning
          component: performance
        annotations:
          summary: "High request latency detected"
          description: "95th percentile latency is {{ $value }}s"
          
      # Circuit breaker open
      - alert: CircuitBreakerOpen
        expr: circuit_breaker_state{state="open"} == 1
        for: 2m
        labels:
          severity: critical
          component: resilience
        annotations:
          summary: "Circuit breaker {{ $labels.breaker_name }} is open"
          description: "Circuit breaker has been open for 2 minutes, indicating persistent failures"

  - name: lazy_loading
    interval: 30s
    rules:
      # Slow agent loading
      - alert: SlowAgentLoading
        expr: agent_load_duration_seconds > 30
        for: 1m
        labels:
          severity: warning
          component: lazy_loader
        annotations:
          summary: "Agent {{ $labels.agent_name }} slow to load"
          description: "Agent took {{ $value }}s to load (SLA: 30s)"
          
      # LazyLoader queue backup
      - alert: LazyLoaderQueueBackup
        expr: lazy_loader_queue_size > 10
        for: 5m
        labels:
          severity: warning
          component: lazy_loader
        annotations:
          summary: "LazyLoader queue is backing up"
          description: "{{ $value }} agents waiting to be loaded"

  - name: hybrid_routing
    interval: 30s
    rules:
      # Cloud LLM unavailable
      - alert: CloudLLMUnavailable
        expr: cloud_llm_available == 0
        for: 5m
        labels:
          severity: critical
          component: llm_routing
        annotations:
          summary: "Cloud LLM endpoint unavailable"
          description: "Cloud LLM has been unavailable for 5 minutes"
          
      # High failover rate
      - alert: HighLLMFailoverRate
        expr: rate(llm_routing_failovers_total[10m]) > 0.1
        for: 10m
        labels:
          severity: warning
          component: llm_routing
        annotations:
          summary: "High LLM failover rate"
          description: "LLM routing failing over {{ $value }} times per second"
          
      # Routing accuracy degraded
      - alert: RoutingAccuracyDegraded
        expr: llm_routing_accuracy < 0.95
        for: 15m
        labels:
          severity: warning
          component: llm_routing
        annotations:
          summary: "LLM routing accuracy below threshold"
          description: "Routing accuracy is {{ $value | humanizePercentage }} (threshold: 95%)"

  - name: observability
    interval: 30s
    rules:
      # ObservabilityHub down
      - alert: ObservabilityHubDown
        expr: up{job="observability_hub"} == 0
        for: 2m
        labels:
          severity: critical
          component: monitoring
        annotations:
          summary: "ObservabilityHub is down"
          description: "Central monitoring system has been down for 2 minutes"
          
      # Metrics collection failing
      - alert: MetricsCollectionFailing
        expr: rate(observability_hub_scrape_failures_total[5m]) > 0.5
        for: 5m
        labels:
          severity: warning
          component: monitoring
        annotations:
          summary: "Metrics collection failing"
          description: "ObservabilityHub failing to collect metrics at {{ $value }} per second"

# Alert routing configuration (for Alertmanager)
# This would be in a separate alertmanager.yml file
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - localhost:9093
  
# Recording rules for performance optimization
recording_rules:
  - name: aggregations
    interval: 30s
    rules:
      # Agent count by status
      - record: agent_count_by_status
        expr: count by (status) (up{job="unified_agents"})
        
      # Memory usage by profile
      - record: memory_usage_by_profile
        expr: sum by (profile) (process_resident_memory_bytes / 1024 / 1024)
        
      # Request rate by agent
      - record: request_rate_by_agent
        expr: rate(http_requests_total[5m])
        
      # LLM routing statistics
      - record: llm_routing_cloud_percentage
        expr: rate(llm_routing_decisions_total{backend="cloud"}[5m]) / rate(llm_routing_decisions_total[5m])
        
      # System health score (0-1)
      - record: system_health_score
        expr: |
          (
            (count(up{job="unified_agents", required="true"} == 1) / count(up{job="unified_agents", required="true"}))
            * 0.5
          ) + (
            (1 - clamp_max(rate(agent_restarts_total[10m]), 1))
            * 0.3
          ) + (
            (gpu_memory_available_mb / gpu_memory_total_mb)
            * 0.2
          )