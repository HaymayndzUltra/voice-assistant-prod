[
  {
    "id": "20240524_dual_machine_optimization",
    "description": "A comprehensive plan to optimize, validate, and harden a dual-machine AI system with 77 containerized agents, focusing on performance, stability, and maintainability.",
    "todos": [
      {
        "text": "PHASE 0: SETUP & PROTOCOL (READ FIRST)\n\n**Explanations:**\nThis initial step contains the user manual for this task plan. It outlines the commands to interact with the plan and the critical safety workflow that must be followed for all subsequent phases.\n\n**Technical Artifacts:**\n**HOW TO USE THIS TASK PLAN (COMMANDS & PROTOCOL)**\n\n**I. COMMANDS:**\n1.  **TO VIEW DETAILS:** `python3 todo_manager.py show 20240524_dual_machine_optimization`\n2.  **TO MARK AS DONE:** `python3 todo_manager.py done 20240524_dual_machine_optimization <step_number>`\n\n**II. WORKFLOW & SAFETY PROTOCOL (CRUCIAL):**\n1.  **FOCUS ON CURRENT STEP:** In each Phase, always read and understand the `IMPORTANT NOTE` first.\n2.  **REVIEW-CONFIRM-PROCEED LOOP:** After completing a Phase, review your work and the next Phase. If your confidence score is below 90%, REPEAT the review.\n\n──────────────────────────────────\nIMPORTANT NOTE: This phase contains the operating manual for the entire plan. Understanding these protocols is mandatory before proceeding to Phase 1. Do not proceed until the current step is complete. Before moving forward, review the completed step and the next one. Repeat the review if your confidence score is below 90%.",
        "done": false
      },
      {
        "text": "PHASE 1: System Discovery & Dependency Analysis\n\n**Explanations:**\nThis phase focuses on mapping the entire system, analyzing all 77 containers, and conducting a deep dive into code dependencies. The goal is to create a complete and accurate picture of the system's current state, which is a mandatory prerequisite for any optimization.\n\n**Technical Artifacts / Tasks:**\n**1. Mandatory System Discovery (RUN FIRST):**\n*CRITICAL: Execute this before any other analysis! The background agent MUST start with system discovery to know exactly where all files are located!* \n```bash\n# Run the system discovery script to map the complete file structure\npython system_discovery_script.py\n\n# This will generate:\n# - system_discovery_report.json (complete system mapping)\n# - analysis_paths.json (specific file paths for analysis)\n```\n\n**2. System Discovery & Container Analysis:**\n*   **Complete File System Mapping:** Scan the primary locations:\n    ```\n    ./docker/                                    # 77 Docker containers\n    ./main_pc_code/agents/                      # MainPC agent source code\n    ./pc2_code/agents/                          # PC2 agent source code\n    ./common/                                   # Shared utilities and base classes\n    ./phase1_implementation/                    # Phase implementation code\n    ./scripts/                                  # Build and utility scripts\n    ./requirements*.txt                         # Root-level requirements\n    ./docker-compose*.yml                       # System-level compose files\n    ```\n*   **Container Discovery and Analysis (`./docker/*/`):**\n    *   Validate each of the 77 Dockerfiles builds successfully.\n    *   Check `docker-compose.yml` files for proper configuration.\n    *   Verify `requirements.txt` files have no conflicting dependencies.\n    *   Test container startup sequences against `startup_config.yaml`.\n    *   Document any containers that fail to build or start.\n    *   Validate health check endpoints respond correctly.\n\n**3. Requirements Discovery & Completion:**\n*   **Agent Code Analysis:** Scan Python files in all target locations (`./docker/*/`, `./main_pc_code/agents/*/`, `./pc2_code/agents/*/`, `./common/*/`, `./phase1_implementation/*/`) to:\n    *   Trace all direct and indirect import statements.\n    *   Identify missing packages not listed in `requirements.txt`.\n    *   Detect unused packages listed but not imported.\n    *   Generate a complete dependency tree for each agent.\n*   **Requirements File Validation:** Check all `requirements.txt` files across the project to:\n    *   Identify containers with missing or incomplete files.\n    *   Generate missing `requirements.txt` files using static analysis.\n    *   Validate all imported packages are properly versioned.\n    *   Create backups of original requirements files before modification.\n*   **Dynamic & Cross-Agent Dependency Mapping:**\n    *   Analyze configuration files (`startup_config*.yaml`, `docker-compose.yml`, `env_config*.sh`) for runtime imports and platform-specific requirements.\n    *   Map shared internal modules from `./common/*/` between agents.\n    *   Create a dependency graph showing inter-agent relationships and validate startup order.\n\n**4. Tooling for Automated Requirements Generation:**\n*   Utilize `pipreqs` for basic requirements generation.\n*   Implement a custom AST parser for complex import patterns.\n*   Use `pip-tools` for dependency resolution and version pinning.\n*   Employ `safety` for security vulnerability scanning.\n\n**5. Requirements Validation & Testing:**\n*   Test-build each container with its generated `requirements.txt`.\n*   Validate all imports work correctly at runtime.\n*   Check for missing system-level dependencies (e.g., apt packages).\n*   Ensure version compatibility across the ecosystem.\n\n──────────────────────────────────\nIMPORTANT NOTE: This discovery phase is the foundation for the entire project. Inaccuracies or incomplete analysis here will lead to failures in subsequent optimization and implementation phases. Do not proceed until the current step is complete. Before moving forward, review the completed step and the next one. Repeat the review if your confidence score is below 90%.",
        "done": false
      },
      {
        "text": "PHASE 2: Optimization Strategy & Remediation Planning\n\n**Explanations:**\nBased on the findings from Phase 1, this phase involves creating a detailed strategy for all optimization and remediation tasks. This includes planning for build time reduction, performance enhancements, and infrastructure hardening. The output of this phase is a clear, prioritized action plan.\n\n**Technical Artifacts / Tasks:**\n**1. Formulate Requirements Optimization Strategy:**\n*   **Duplicate Analysis:** Analyze all 77 `requirements.txt` files to find duplicate packages and version conflicts. Plan a `requirements.common.txt` for packages appearing in 10+ containers.\n*   **Base Image Optimization:** Design optimized base images with common dependencies pre-installed. Group containers by requirement similarity (e.g., ML, API) and plan for multi-stage builds.\n*   **Dependency Consolidation:** Identify containers with 80%+ requirement overlap for potential merging. Define requirement profiles (minimal, standard, full) and plan the removal of unused dependencies.\n*   **Build Time Optimization:** Develop a strategy for Docker layer caching, optimized requirement installation order, and using `.dockerignore` to reduce build context.\n\n**2. Plan Performance Optimization Implementation:**\n*   **GPU Memory (PC2):** Plan the implementation of int8 quantization for NLLB models, moving TTS to CPU, and enabling predictive model unloading.\n*   **Memory Leaks:** Outline the steps to fix missing `torch.cuda.empty_cache()` calls, implement context-managed tensor lifecycles, and add periodic queue purging.\n*   **Algorithm Optimization:** Plan the replacement of O(N²) image comparison, caching of spaCy models, fixing N+1 DB queries, and converting sync I/O to async.\n\n**3. Plan Infrastructure Hardening & Efficiency Improvements:**\n*   **Code Quality:** Create a task list to fix bare exceptions, resolve circular imports, standardize port allocation, replace hard-coded credentials, and implement connection pooling.\n*   **Runtime Optimization:** Plan the right-sizing of container resources, implementation of ZMQ connection pooling, and optimization of message serialization.\n\n**4. Review and Prioritize:**\n*   Formalize the action plan according to the specified priorities:\n    1.  **CRITICAL:** GPU memory optimization for PC2.\n    2.  **HIGH:** Requirements deduplication and build optimization.\n    3.  **HIGH:** Memory leak fixes and monitoring.\n    4.  **MEDIUM:** Algorithm and runtime optimizations.\n    5.  **MEDIUM:** Infrastructure hardening and monitoring.\n    6.  **LOW:** Documentation and maintenance procedures.\n\n──────────────────────────────────\nIMPORTANT NOTE: A clear and well-defined strategy is crucial for success. Rushing into implementation without a solid plan will result in wasted effort and potential system instability. Ensure all stakeholders agree on the priorities and planned actions. Do not proceed until the current step is complete. Before moving forward, review the completed step and the next one. Repeat the review if your confidence score is below 90%.",
        "done": false
      },
      {
        "text": "PHASE 3: Code & Infrastructure Implementation\n\n**Explanations:**\nThis is the execution phase where the strategies defined in Phase 2 are implemented. It involves hands-on coding to optimize algorithms, fix memory leaks, improve code quality, and harden the system's infrastructure.\n\n**Technical Artifacts / Tasks:**\n**1. Implement GPU Memory Optimizations (CRITICAL - PC2):**\n*   Implement int8 quantization for NLLB translation models.\n*   Move TTS inference from GPU to CPU containers to save ~1.8GB VRAM.\n*   Enable predictive model unloading in `vram_optimizer_agent`.\n*   Add GPU memory monitoring with alerts for usage >90%.\n*   Optimize model sharing between containers to reduce memory duplication.\n\n**2. Implement Memory Leak Fixes:**\n*   Fix `translation_service.py` by adding missing `torch.cuda.empty_cache()` calls.\n*   Implement context-managed tensor lifecycles for all GPU tensors.\n*   Add periodic queue purging for the `dream_world_agent`'s `asyncio.Queue` retention issue.\n*   Review and fix all model loading/unloading sequences for proper cleanup.\n*   Add memory profiling for long-running containers.\n\n**3. Implement Algorithm Optimizations:**\n*   Replace O(N²) image comparison in `face_recognition_agent` with a KD-tree implementation.\n*   Cache spaCy model loading in `learning_opportunity_detector` at the module level.\n*   Fix N+1 database queries in `memory_orchestrator_service.py` with batch operations.\n*   Convert synchronous I/O operations to async/await patterns where applicable.\n*   Implement efficient data serialization (e.g., msgpack or orjson) instead of standard JSON.\n\n**4. Implement Infrastructure Hardening & Efficiency:**\n*   **Code Quality:**\n    *   Refactor the 4,117 identified bare `except:` handlers to be specific.\n    *   Resolve circular import dependencies between core modules.\n    *   Standardize port allocation to fix conflicts (e.g., 5556/5581).\n    *   Replace hard-coded credentials with environment variables.\n    *   Implement connection pooling for Redis and database connections.\n    *   Add LRU caching for frequently accessed data.\n*   **Runtime Optimization:**\n    *   Right-size container resource limits and requests based on profiling data.\n    *   Use shared volumes for common data like models and configs.\n    *   Implement ZMQ connection pooling and optimize message serialization.\n\n──────────────────────────────────\nIMPORTANT NOTE: This phase involves significant changes to the codebase and infrastructure. Work on a dedicated feature branch and commit changes incrementally. Thoroughly unit-test each change before moving to the next. Do not proceed until the current step is complete. Before moving forward, review the completed step and the next one. Repeat the review if your confidence score is below 90%.",
        "done": false
      },
      {
        "text": "PHASE 4: Test Suite & Monitoring Infrastructure Development\n\n**Explanations:**\nTo validate the optimizations from Phase 3 and ensure long-term stability, this phase focuses on creating a comprehensive, automated test suite and robust monitoring infrastructure.\n\n**Technical Artifacts / Tasks:**\n**1. Generate Automated Test Scripts:**\n*   Create scripts for individual container health checks.\n*   Develop tests for cross-machine ZMQ communication validation.\n*   Implement tests for GPU memory usage monitoring and alerting.\n*   Build a load test for the translation pipeline to validate performance under high VRAM usage on PC2.\n*   Create a test to validate the dependency chain during system startup and shutdown.\n*   Establish performance regression tests for all optimization implementations.\n*   Develop benchmarks for build time and resource usage to track improvements.\n\n**2. Create Monitoring Infrastructure:**\n*   Set up monitoring dashboards to provide real-time visibility into:\n    *   GPU utilization on both RTX 4090 (MainPC) and RTX 3060 (PC2).\n    *   Memory usage patterns and leak detection metrics.\n    *   Inter-container communication latency.\n    *   Error rate tracking across all 77 agents.\n    *   Build times and resource consumption over time.\n    *   Container startup sequence performance.\n\n──────────────────────────────────\nIMPORTANT NOTE: A system is only as reliable as its tests and monitoring. Ensure the test suite has high coverage of critical paths and that the monitoring dashboards provide actionable insights, not just data. Do not proceed until the current step is complete. Before moving forward, review the completed step and the next one. Repeat the review if your confidence score is below 90%.",
        "done": false
      },
      {
        "text": "PHASE 5: Validation, Reporting, and Final Review\n\n**Explanations:**\nThis final phase involves validating the entire effort against the defined success criteria, generating all required reports and deliverables, and preparing the system for production deployment.\n\n**Technical Artifacts / Tasks:**\n**1. Final Validation Against Success Criteria:**\n*   Confirm all 77 containers build and start successfully.\n*   Verify PC2 GPU utilization is reduced to <85% during translation bursts.\n*   Confirm memory leaks are eliminated via stable long-running performance tests.\n*   Ensure zero startup failures due to dependency issues.\n*   Validate that comprehensive monitoring and alerting are fully operational.\n*   Measure and confirm build times are reduced by 40%+.\n*   Measure and confirm storage footprint is reduced by 30%+.\n*   Measure and confirm container startup time is improved by 50%+.\n\n**2. Generate Specific Deliverables:**\n*   **Container Optimization Report:**\n    *   Final container validation report (build/startup status for all 77 agents).\n    *   Complete dependency mapping and missing requirements report.\n    *   Requirements deduplication analysis and base image optimization strategy with final size reduction metrics.\n    *   Build time optimization report with before/after comparisons.\n*   **Performance Optimization Report:**\n    *   GPU memory optimization results (PC2 usage <85% target).\n    *   Memory leak fix validation with monitoring data.\n    *   Algorithm optimization performance benchmarks.\n*   **Production-Ready Infrastructure Documentation:**\n    *   Documentation for the automated test suite.\n    *   Configuration files and guides for the monitoring dashboards.\n*   **Technical Debt Remediation Plan:**\n    *   Finalized technical debt remediation plan with a priority matrix.\n    *   Updated architectural documentation.\n    *   A best practices guide for future container development.\n    *   Maintenance procedures for ongoing optimization.\n\n──────────────────────────────────\nIMPORTANT NOTE: This is the final acceptance phase. All deliverables must be complete and all success criteria must be met before the project is considered finished. A final peer review of the code, documentation, and reports is mandatory. Do not proceed until the current step is complete. Before moving forward, review the completed step and the next one. Repeat the review if your confidence score is below 90%.",
        "done": false
      }
    ],
    "status": "in_progress",
    "created": "2024-05-24T12:00:00Z",
    "updated": "2024-05-24T12:00:00Z"
  }
]