[
  {
    "id": "20250807_hub_consolidation_strategy",
    "description": "Action plan to implement the consolidated multi-hub architecture, focusing on GPU resource management, optimized memory architecture, refined deployment strategies, and enhanced observability.",
    "todos": [
      {
        "text": "PHASE 0: SETUP & PROTOCOL (READ FIRST)\n\n**Explanations:**\nThis initial step contains the user manual for this task plan. It outlines the commands to interact with the plan and the critical safety workflow that must be followed for all subsequent phases.\n\n**Technical Artifacts:**\n**I. CORE BEHAVIORAL MANDATES**\n*   **Execute Serially:** You must execute the phases in the prescribed order. Do not skip phases.\n*   **Verify Each Step:** After completing a phase, verify its success before marking it as 'done'.\n*   **Consult Source:** This plan is an interpretation of the source document. If any ambiguity arises, the source document is the ground truth.\n\n**II. HOW TO USE THIS TASK PLAN**\n*   **To Show the Plan:** `python3 todo_manager.py show 20250807_hub_consolidation_strategy`\n*   **To Mark a Phase Done:** `python3 todo_manager.py done 20250807_hub_consolidation_strategy <phase_number>` (e.g., `... done 20250807_hub_consolidation_strategy 0`)\n\n**Concluding Step: Update Plan Status**\nTo officially conclude this setup phase and update the plan's state, run the following commands. This ensures the task manager knows you are ready to proceed to the first technical phase.\n*   **Review Plan Details:** `python3 todo_manager.py show 20250807_hub_consolidation_strategy`\n*   **Mark This Phase as Complete:** `python3 todo_manager.py done 20250807_hub_consolidation_strategy 0`\n\n──────────────────────────────────\nIMPORTANT NOTE: This phase contains the operating manual for the entire plan. Understanding these protocols is mandatory before proceeding to Phase 1. Do not proceed until the current step is complete. Before moving forward, review the completed step and the next one. Repeat the review if your confidence score is below 90%.",
        "done": true
      },
      {
        "text": "PHASE 1: Implement GPU Lease API for Resource Contention\n\n**Explanations:**\nTo prevent GPU Out-Of-Memory (OOM) bursts and manage VRAM contention, this phase implements a centralized GPU Lease API within the ModelOps Coordinator (MOC). All other services requiring GPU resources must acquire a lease before allocating memory on the device.\n\n**Technical Artifacts / Tasks:**\n**I. Define the GPU Lease API (`.proto`):**\nUpdate the `model_ops.proto` file with the following service definitions.\n```proto\nsyntax = \"proto3\";\npackage modelops;\n\nservice ModelOps {\n  // Existing endpoints …\n  rpc AcquireGpuLease (GpuLeaseRequest)  returns (GpuLeaseReply);\n  rpc ReleaseGpuLease (GpuLeaseRelease)  returns (GpuLeaseReleaseAck);\n}\n\nmessage GpuLeaseRequest  {\n  string client            = 1;\n  string model_name        = 2;\n  int64  vram_estimate_mb  = 3;\n  int32  priority          = 4;   // 1-highest\n  int32  ttl_seconds       = 5;\n}\n\nmessage GpuLeaseReply {\n  bool   granted           = 1;\n  string lease_id          = 2;\n  int64  vram_reserved_mb  = 3;\n  string reason            = 4;\n  int32  retry_after_ms    = 5;\n}\n\nmessage GpuLeaseRelease   { string lease_id = 1; }\nmessage GpuLeaseReleaseAck{ bool   success  = 1; }\n```\n\n**II. Implement the gRPC Server Logic in MOC:**\nAdd the following async gRPC server implementation to the ModelOps Coordinator.\n```python\n# gpu_lease_server.py\nimport asyncio, time, grpc\nfrom concurrent import futures\nimport model_ops_pb2 as pb2\nimport model_ops_pb2_grpc as pb2_grpc\n\nclass LeaseState:\n    def __init__(self, total_mb=24_000, reserve=0.9):\n        self.cap_mb = int(total_mb * reserve)\n        self.used_mb = 0\n        self.leases = {}\n        self.lock = asyncio.Lock()\n\nclass ModelOps(pb2_grpc.ModelOpsServicer):\n    def __init__(self, state: LeaseState):\n        self.state = state\n\n    async def AcquireGpuLease(self, req, ctx):\n        async with self.state.lock:\n            if self.state.used_mb + req.vram_estimate_mb <= self.state.cap_mb:\n                lid = f\"{int(time.time()*1000)}_{req.client}\"\n                self.state.leases[lid] = (req.vram_estimate_mb, time.time()+req.ttl_seconds)\n                self.state.used_mb += req.vram_estimate_mb\n                return pb2.GpuLeaseReply(granted=True, lease_id=lid,\n                                         vram_reserved_mb=req.vram_estimate_mb)\n            return pb2.GpuLeaseReply(granted=False, reason=\"Insufficient VRAM\",\n                                     retry_after_ms=250)\n\n    async def ReleaseGpuLease(self, req, ctx):\n        async with self.state.lock:\n            mb, _ = self.state.leases.pop(req.lease_id, (0,0))\n            self.state.used_mb -= mb\n        return pb2.GpuLeaseReleaseAck(success=True)\n```\n\n**III. Integrate the Lease Client into GPU-Using Agents:**\nAll GPU-using agents (e.g., APC, RTAP-GPU) must use the following client wrapper to acquire a lease before any CUDA operations.\n```python\n# gpu_lease_client.py\nimport grpc, time, model_ops_pb2 as pb2, model_ops_pb2_grpc as pb2_grpc\n\nclass GpuLeaseClient:\n    def __init__(self, addr=\"mainpc:50051\"):\n        self.stub = pb2_grpc.ModelOpsStub(grpc.insecure_channel(addr))\n        self.lease_id = None\n\n    def acquire(self, client, model, mb, prio=2, ttl=30):\n        backoff = 0.25\n        for _ in range(6):\n            rep = self.stub.AcquireGpuLease(pb2.GpuLeaseRequest(\n                client=client, model_name=model, vram_estimate_mb=mb,\n                priority=prio, ttl_seconds=ttl))\n            if rep.granted:\n                self.lease_id = rep.lease_id\n                return True\n            time.sleep(backoff); backoff = min(backoff*2, 2.0)\n        return False\n\n    def release(self):\n        if self.lease_id:\n            self.stub.ReleaseGpuLease(pb2.GpuLeaseRelease(lease_id=self.lease_id))\n            self.lease_id = None\n```\n\n**Concluding Step: Update Plan Status**\nAfter all technical tasks in this phase are successfully completed, run the following commands to mark this phase as done and prepare for the next one.\n*   **Review Plan Details:** `python3 todo_manager.py show 20250807_hub_consolidation_strategy`\n*   **Mark This Phase as Complete:** `python3 todo_manager.py done 20250807_hub_consolidation_strategy 1`\n\n──────────────────────────────────\nIMPORTANT NOTE: This GPU lease mechanism is a foundational change affecting all GPU-intensive services. Proper integration is mandatory to prevent resource contention and system instability.",
        "done": true
      },
      {
        "text": "PHASE 2: Optimize Memory Architecture & Data Flow\n\n**Explanations:**\nThis phase optimizes the memory architecture by deploying a read-through proxy for the Memory Fusion Hub (MFH) on MainPC and centralizing all embedding storage within the authoritative MFH on PC2.\n\n**Technical Artifacts / Tasks:**\n1.  **Deploy MFH Proxy:** Develop and deploy the `mfh-proxy` on MainPC. This proxy will be a thin, stateless read-through cache.\n2.  **Configure Proxy Behavior:** The proxy must serve hot reads from its local RAM. All writes and cache misses must be forwarded via gRPC to the authoritative MFH instance on PC2.\n3.  **Centralize Embeddings:** Refactor the Affective Processing Center (APC) and any other relevant services to stop using their own storage for embeddings. All writes and reads of embeddings must go through MFH, making it the single source-of-truth.\n4.  **Add Metrics:** The `mfh-proxy` must publish cache-hit/miss metrics to the Unified Observability Center (UOC) to enable future adaptive TTL tuning.\n\n**Concluding Step: Update Plan Status**\nAfter all technical tasks in this phase are successfully completed, run the following commands to mark this phase as done and prepare for the next one.\n*   **Review Plan Details:** `python3 todo_manager.py show 20250807_hub_consolidation_strategy`\n*   **Mark This Phase as Complete:** `python3 todo_manager.py done 20250807_hub_consolidation_strategy 2`\n\n──────────────────────────────────\nIMPORTANT NOTE: Centralizing data ownership in MFH improves data consistency and reduces memory footprints across services. The proxy on MainPC ensures low-latency access for local services.",
        "done": true
      },
      {
        "text": "PHASE 3: Implement Split-Service Deployment Strategy\n\n**Explanations:**\nThis phase implements the new physical deployment layout, splitting services across MainPC and PC2 to optimize for their workload profiles. This includes refactoring the Real-Time Audio Pipeline (RTAP).\n\n**Technical Artifacts / Tasks:**\n**I. Adhere to Optimal Hub Placement:**\nAll deployment scripts must be updated to match the following distribution:\n| Hub | Primary workload profile | Optimal host | Notes |\n| :--- | :--- | :--- | :--- |\n| Memory Fusion Hub (MFH) | CPU-bound, large-RAM KV | PC2 (authoritative) | Read-through cache/proxy on MainPC |\n| ModelOps Coordinator (MOC) | GPU scheduling & model lifecycle | MainPC | Central GPU control plane |\n| Affective Processing Center (APC) | Real-time multimodal deep models | MainPC | Low-latency, GPU-intensive |\n| Real-Time Audio Pipeline | Low-latency DSP + ML | Split → rtap-pre (PC2) / rtap-gpu (MainPC) | Pre-proc on PC2, inference on MainPC |\n| Unified Observability Center (UOC) | Telemetry ingest, alerting | PC2 (edge) + MainPC (central) | NATS JetStream cluster spans both |\n\n**II. Refactor Real-Time Audio Pipeline (RTAP):**\nSplit the existing RTAP service into two distinct components:\n*   `rtap-pre`: A service running on PC2 responsible for audio capture and pre-processing (DSP).\n*   `rtap-gpu`: A service running on MainPC responsible for GPU-based inference (e.g., STT).\nThe hand-off of audio frames from `rtap-pre` to `rtap-gpu` will occur over the network (e.g., ZMQ or UDP).\n\n**III. Update Docker Compose Configuration:**\nImplement the new deployment strategy in `docker-compose.dist.yaml`.\n```yaml\nversion: \"3.9\"\nservices:\n  # PC2-resident services\n  mfh:\n    build: ./memory_fusion_hub\n    deploy: { resources: { reservations: { devices: [] } } }\n    networks: [core_net]\n    environment: { ROLE: \"authoritative\" }\n\n  rtap-pre:\n    build: ./rt_audio_pipeline/preproc\n    networks: [core_net]\n\n  uoc-edge:\n    build: ./unified_observability_center\n    environment: { ROLE: \"edge\" }\n    networks: [core_net]\n\n  # MainPC-resident services\n  moc:\n    build: ./model_ops_coordinator\n    deploy:\n      resources: { reservations: { devices: [{ capabilities: [gpu] }] } }\n    networks: [core_net]\n\n  apc:\n    build: ./affective_processing_center\n    depends_on: [moc]\n    deploy:\n      resources: { reservations: { devices: [{ capabilities: [gpu] }] } }\n    networks: [core_net]\n\n  rtap-gpu:\n    build: ./rt_audio_pipeline/gpu\n    depends_on: [moc]\n    deploy:\n      resources: { reservations: { devices: [{ capabilities: [gpu] }] } }\n    networks: [core_net]\n\n  mfh-proxy:\n    build: ./memory_fusion_hub/proxy\n    depends_on: [mfh]\n    networks: [core_net]\n    environment:\n      TARGET_MFH_HOST: \"mfh\"\n      CACHE_TTL_SEC: \"60\"\n\n  uoc-central:\n    build: ./unified_observability_center\n    environment: { ROLE: \"central\" }\n    networks: [core_net]\n\nnetworks:\n  core_net: { driver: bridge }\n```\n\n**Concluding Step: Update Plan Status**\nAfter all technical tasks in this phase are successfully completed, run the following commands to mark this phase as done and prepare for the next one.\n*   **Review Plan Details:** `python3 todo_manager.py show 20250807_hub_consolidation_strategy`\n*   **Mark This Phase as Complete:** `python3 todo_manager.py done 20250807_hub_consolidation_strategy 3`\n\n──────────────────────────────────\nIMPORTANT NOTE: This is a significant architectural refactoring. Thoroughly test inter-service communication, especially the new `rtap-pre` to `rtap-gpu` link, to ensure latency budgets are met.",
        "done": false
      },
      {
        "text": "PHASE 4: Harden Network & Observability Infrastructure\n\n**Explanations:**\nThis phase improves the resilience and insight of the entire system by clustering the NATS event bus, enforcing stricter network policies, and adding proactive latency monitoring.\n\n**Technical Artifacts / Tasks:**\n1.  **Cluster NATS JetStream:** Configure and deploy NATS as a cluster with at least one node per host (MainPC and PC2). This provides a resilient, cross-machine event bus.\n2.  **Deploy Distributed UOC:** Deploy the Unified Observability Center with `uoc-central` on MainPC and `uoc-edge` on PC2. Both instances should connect to and consume from the same clustered NATS stream.\n3.  **Deprecate Cross-Machine ZMQ:** Audit and refactor all services to eliminate any ZMQ communication that crosses machine boundaries. ZMQ should be strictly used for intra-machine Inter-Process Communication (IPC). Use the NATS cluster for all cross-machine messaging.\n4.  **Implement Latency Probes:** Add synthetic-latency probes that periodically send messages through the system and report end-to-end timings to the UOC. Configure the UOC's healing engine to trigger alerts or actions based on sustained latency spikes.\n\n**Concluding Step: Update Plan Status**\nAfter all technical tasks in this phase are successfully completed, run the following commands to mark this phase as done and prepare for the next one.\n*   **Review Plan Details:** `python3 todo_manager.py show 20250807_hub_consolidation_strategy`\n*   **Mark This Phase as Complete:** `python3 todo_manager.py done 20250807_hub_consolidation_strategy 4`\n\n──────────────────────────────────\nIMPORTANT NOTE: A clustered NATS bus is the backbone of the new architecture. Enforcing network policies and adding proactive monitoring are crucial for maintaining system-wide stability and performance.",
        "done": false
      },
      {
        "text": "PHASE 5: Finalize QoS and CI/CD Policies\n\n**Explanations:**\nThis final phase implements cross-cutting quality-of-service policies and integrates critical performance checks into the CI/CD pipeline to prevent regressions.\n\n**Technical Artifacts / Tasks:**\n1.  **Create Shared QoS Library:** Develop a shared Python library, `core_qos`, that provides unified throttling and quality-of-service mechanisms. Integrate this library into the Affective Processing Center (APC) and the Real-Time Audio Pipeline (RTAP).\n2.  **Implement VRAM Fragmentation Test:** Create a test for the ModelOps Coordinator that measures VRAM fragmentation after a series of model load/unload cycles.\n3.  **Enforce CI Gate:** Integrate the VRAM fragmentation test into the CI/CD pipeline. The pipeline must be configured to **block merges** if the test reports VRAM waste of 10% or more.\n\n**Concluding Step: Update Plan Status**\nAfter all technical tasks in this phase are successfully completed, run the following commands to mark this phase as done and prepare for the next one.\n*   **Review Plan Details:** `python3 todo_manager.py show 20250807_hub_consolidation_strategy`\n*   **Mark This Phase as Complete:** `python3 todo_manager.py done 20250807_hub_consolidation_strategy 5`\n\n──────────────────────────────────\nIMPORTANT NOTE: These policies are essential for ensuring the long-term health and performance of the system. The CI gate for VRAM fragmentation is a critical guard against performance degradation over time.",
        "done": false
      }
    ],
    "status": "in_progress",
    "created": "2024-05-24T14:15:00Z",
    "updated": "2025-08-08T12:49:52.427567+08:00"
  }
]