[
  {
    "id": "cross_gpu_scheduler_setup",
    "description": "Action plan to set up the Cross-Machine GPU Scheduler service, including scaffolding, code generation, testing, and configuration updates.",
    "todos": [
      {
        "text": "PHASE 0: SETUP & PROTOCOL (READ FIRST)\n\n**Explanations:**\nThis initial step contains the user manual for this task plan. It outlines the commands to interact with the plan and the critical safety workflow that must be followed for all subsequent phases.\n\n**Technical Artifacts:**\n**I. CORE BEHAVIORAL MANDATES (FOR THE EXECUTING AI - READ FIRST)**\n1.  **Validate Assumptions:** Before starting, validate your assumptions about the task using the provided details.\n2.  **Clarify Ambiguity:** If information is ambiguous or missing, seek clarification before proceeding.\n3.  **Highlight Conflicts:** If you detect errors or conflicts in the data, highlight them immediately and suggest alternatives.\n4.  **Explain Rationale:** During execution, explain the rationale for each step taken.\n5.  **Review Before Submission:** Before submitting the final output, review the entire process to correct any errors or flawed assumptions.\n\n**II. HOW TO USE THIS TASK PLAN (COMMANDS & PROTOCOL)**\n\n1.  **COMMANDS:**\n    *   **TO VIEW DETAILS:** `python3 todo_manager.py show cross_gpu_scheduler_setup`\n    *   **TO MARK AS DONE:** `python3 todo_manager.py done cross_gpu_scheduler_setup <step_number>`\n\n2.  **WORKFLOW & SAFETY PROTOCOL (CRUCIAL):**\n    *   **FOCUS ON CURRENT STEP:** In each Phase, always read and understand the `IMPORTANT NOTE` first.\n    *   **REVIEW-CONFIRM-PROCEED LOOP:** After completing a Phase, review your work and the next Phase. If your confidence score is below 90%, REPEAT the review.\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nIMPORTANT NOTE: This phase contains the operating manual for the entire plan. Understanding these protocols is mandatory before proceeding to Phase 1. Do not proceed until the current step is complete. Before moving forward, review the completed step and the next one. Repeat the review if your confidence score is below 90%.",
        "done": false
      },
      {
        "text": "PHASE 1: Cross-Machine GPU Scheduler Setup\n\n**Explanations:**\nThis phase executes the one-shot script to create, configure, and test the new Cross-Machine GPU Scheduler service. It involves creating all necessary files, generating code, running tests, and updating system configuration files.\n\n**Technical Artifacts / Tasks:**\n\n**1. Scaffold Service Directory & Files:**\nCreate the service directory structure and populate it with the necessary source code, configuration, and documentation files.\n\n```bash\nset -euo pipefail\nROOT=\"$(pwd)\"\nSERVICE_DIR=\"services/cross_gpu_scheduler\"\nCONFIG_FILE=\"main_pc_code/config/startup_config.yaml\"\n\necho \"ðŸ”§ 1/7  Scaffolding service directory...\"\nmkdir -p \"$SERVICE_DIR\" tests\n\ncat >\"$SERVICE_DIR/requirements.txt\"<<'REQ'\ngrpcio\ngrpcio-tools\nprometheus-client\npynvml\nuvloop\nREQ\n\ncat >\"$SERVICE_DIR/Dockerfile\"<<'DOCK'\nFROM nvidia/cuda:12.4.0-runtime-ubuntu22.04\nENV PYTHONUNBUFFERED=1\nRUN apt-get update && \\\n    apt-get install -y python3 python3-pip && \\\n    rm -rf /var/lib/apt/lists/*\nCOPY requirements.txt /tmp/\nRUN pip3 install --no-cache-dir -r /tmp/requirements.txt\nCOPY . /app\nWORKDIR /app\nCMD [\"python3\", \"app.py\"]\nDOCK\n\ncat >\"$SERVICE_DIR/scheduler.proto\"<<'PROTO'\nsyntax = \"proto3\";\npackage scheduler;\n\nservice GPUScheduler {\n  rpc Query (ScheduleRequest) returns (ScheduleReply);\n}\n\nmessage ScheduleRequest {\n  string job_id = 1;\n}\n\nmessage ScheduleReply {\n  int32 target_gpu = 1;\n}\nPROTO\n\ncat >\"$SERVICE_DIR/app.py\"<<'PY'\nimport asyncio, os, grpc, logging\nimport pynvml\nfrom prometheus_client import start_http_server, Gauge\nimport scheduler_pb2 as pb\nimport scheduler_pb2_grpc as pb_grpc\n\nlogging.basicConfig(level=logging.INFO)\nLOGGER = logging.getLogger(\"gpu-scheduler\")\n\nGPU_UTIL = Gauge(\"gpu_utilisation_percent\",\n                 \"GPU utilisation\", [\"host\", \"index\"])\n\nclass Scheduler(pb_grpc.GPUSchedulerServicer):\n    def __init__(self) -> None:\n        pynvml.nvmlInit()\n        LOGGER.info(\"NVML initialised with %d GPU(s)\",\n                    pynvml.nvmlDeviceGetCount())\n\n    async def Query(self, request: pb.ScheduleRequest,\n                    context: grpc.aio.ServicerContext) -> pb.ScheduleReply:\n        \"\"\"Pick the least-busy GPU and return its index.\"\"\"\n        idx = min(\n            range(pynvml.nvmlDeviceGetCount()),\n            key=lambda i: pynvml.nvmlDeviceGetUtilizationRates(\n                pynvml.nvmlDeviceGetHandleByIndex(i)\n            ).gpu\n        )\n        util = pynvml.nvmlDeviceGetUtilizationRates(\n            pynvml.nvmlDeviceGetHandleByIndex(idx)\n        ).gpu\n        host = os.getenv(\"HOSTNAME\", \"local\")\n        GPU_UTIL.labels(host, idx).set(util)\n        LOGGER.debug(\"Job %s scheduled on GPU %d (util=%d%%)\",\n                     request.job_id, idx, util)\n        return pb.ScheduleReply(target_gpu=idx)\n\nasync def serve() -> None:\n    server = grpc.aio.server()\n    pb_grpc.add_GPUSchedulerServicer_to_server(Scheduler(), server)\n    server.add_insecure_port(\"[::]:7155\")          # gRPC\n    start_http_server(8000)                        # Prometheus\n    LOGGER.info(\"GPU Scheduler running on :7155  (metrics :8000)\")\n    await server.start()\n    await server.wait_for_termination()\n\nif __name__ == \"__main__\":\n    asyncio.run(serve())\nPY\n\ncat >\"tests/test_scheduler_stub.py\"<<'PYT'\n\"\"\"Sanity-check import & stub generation.\"\"\"\nimport importlib, pathlib, subprocess, sys\n\ndef test_proto_generated():\n    pkg = pathlib.Path(\"services/cross_gpu_scheduler\")\n    proto = pkg/\"scheduler.proto\"\n    assert proto.exists()\n    # Re-generate â€” should succeed and be idempotent\n    subprocess.check_call([\n        sys.executable, \"-m\", \"grpc_tools.protoc\",\n        \"-I\", str(pkg), \"--python_out\", str(pkg),\n        \"--grpc_python_out\", str(pkg), str(proto)\n    ])\n    importlib.import_module(\"services.cross_gpu_scheduler.scheduler_pb2\")\nPYT\n\ncat >\"$SERVICE_DIR/README.md\"<<'MD'\n# Cross-Machine GPU Scheduler\n\nLightweight gRPC service that assigns jobs to the least-busy GPU across **MainPC**  \nand **PC2**. Exposes Prometheus metrics at :8000 and a `GPUScheduler.Query` RPC  \nat :7155.\n\nSee `scheduler.proto` for API.\nMD\n```\n\n**2. Generate gRPC Stubs:**\nCompile the `.proto` file to generate the Python gRPC stubs.\n\n```bash\nSERVICE_DIR=\"services/cross_gpu_scheduler\"\necho \"ðŸ“œ 2/7  Generating gRPC stubs...\"\npython3 -m grpc_tools.protoc -I \"$SERVICE_DIR\" \\\n  --python_out \"$SERVICE_DIR\" --grpc_python_out \"$SERVICE_DIR\" \\\n  \"$SERVICE_DIR/scheduler.proto\"\n```\n\n**3. Run Unit Tests:**\nInstall testing dependencies and run the unit tests to verify the generated stubs.\n\n```bash\necho \"ðŸ§ª 3/7  Running unit tests...\"\npython3 -m pip install --quiet pytest\npytest -q\n```\n\n**4. Docker Build Smoke-Test:**\nBuild the Docker image to ensure the Dockerfile and dependencies are correct.\n\n```bash\nSERVICE_DIR=\"services/cross_gpu_scheduler\"\necho \"ðŸ³ 4/7  Docker build smoke-test...\"\ndocker build -t cross_gpu_scheduler:dev \"$SERVICE_DIR\"\n```\n\n**5. Update Startup Configuration:**\nExecute a Python script to programmatically add the new scheduler service to the `startup_config.yaml` file.\n\n```bash\necho \"ðŸ—‚ 5/7  Injecting agent entry into main_pc_code/config/startup_config.yaml ...\"\npython3 - <<'PY'\nimport yaml, sys, pathlib, textwrap\ncfg_path = pathlib.Path(\"main_pc_code/config/startup_config.yaml\")\ndata = yaml.safe_load(cfg_path.read_text())\n\nagent_name = \"CrossMachineGPUScheduler\"\nagent_def  = {\n    \"script_path\": \"services/cross_gpu_scheduler/app.py\",\n    \"port\": \"${PORT_OFFSET}+7155\",\n    \"health_check_port\": \"${PORT_OFFSET}+8155\",\n    \"required\": True,\n    \"dependencies\": [\"ModelManagerSuite\", \"ObservabilityHub\"]\n}\n\n# 1. Add to top-level agent_groups â†’ gpu_infrastructure (create if absent)\ngroup = data.setdefault(\"agent_groups\", {}).setdefault(\"gpu_infrastructure\", {})\nif agent_name not in group:\n    group[agent_name] = agent_def\n\n# 2. Add docker_groups entry (gpu_scheduler)\ndg = data.setdefault(\"docker_groups\", {})\ndg.setdefault(\"gpu_scheduler\", {\n    \"description\": \"Cross-machine GPU load balancer\",\n    \"agents\": [agent_name]\n})\n\ncfg_path.write_text(textwrap.dedent(yaml.dump(data, sort_keys=False)))\nprint(\"Config updated âœ”\")\nPY\n```\n\n**6. YAML Lint Check:**\nValidate that the modified `startup_config.yaml` is still a valid YAML file.\n\n```bash\necho \"ðŸ“œ 6/7  YAML lint check...\"\npython3 - <<'PY'\nimport yaml, sys, pathlib\nyaml.safe_load(pathlib.Path(\"main_pc_code/config/startup_config.yaml\").read_text())\nprint(\"startup_config.yaml valid YAML âœ”\")\nPY\n```\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nIMPORTANT NOTE: This phase contains multiple steps that modify the file system and configuration. Execute each sub-task sequentially and verify its output before proceeding to the next. The final step is to confirm that all artifacts are ready. Do not proceed until the current step is complete. Before moving forward, review the completed step and the next one. Repeat the review if your confidence score is below 90%.",
        "done": false
      }
    ],
    "status": "in_progress",
    "created": "2024-05-24T12:00:00Z",
    "updated": "2024-05-24T12:00:00Z"
  }
]