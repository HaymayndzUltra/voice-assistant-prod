# MainPC Machine-Specific Overrides
# GPU-focused configuration for RTX 4090 system

version: "3.0"
machine: "mainpc"
description: "MainPC overrides for RTX 4090 GPU system"

# MainPC-specific environment variables
global_settings:
  environment:
    MACHINE_ROLE: MAIN_PC
    CUDA_VISIBLE_DEVICES: "0"
    TORCH_CUDA_ARCH_LIST: "8.6"  # RTX 4090 architecture
    GPU_MEMORY_FRACTION: "0.9"
    VRAM_LIMIT_MB: "22000"  # RTX 4090 has 24GB
    MODEL_CACHE_DIR: "/app/models"
    HUGGINGFACE_CACHE: "/app/cache/huggingface"
    
  # Higher resource limits for MainPC
  resource_limits:
    cpu_percent: 85
    memory_mb: 8192
    max_threads: 16
    gpu_memory_mb: 22000
    
  # MainPC network settings
  network:
    subnet: "172.20.0.0/16"
    gateway: "172.20.0.1"

# Override specific agent configurations for MainPC
agent_overrides:
  # GPU infrastructure gets priority resources
  gpu_infrastructure:
    ModelManagerAgent:
      config:
        gpu_layers: -1  # Use full GPU
        context_length: 4096
        batch_size: 512
        max_models: 3
        
    VRAMOptimizerAgent:
      config:
        memory_threshold: 0.85
        optimization_interval: 60
        aggressive_cleanup: true
        
    GGUFModelManager:
      config:
        preload_models: ["llama-2-7b-chat", "mistral-7b-instruct"]
        offload_layers: 35
        
  # Core services get enhanced configurations
  core_services:
    SystemDigitalTwin:
      config:
        cache_size_mb: 1024
        prediction_window: 300
        metrics_retention_days: 30
        
    ServiceRegistry:
      config:
        backend: redis  # Use Redis for persistence on MainPC
        health_check_parallel: true
        
  # Reasoning services optimized for GPU
  reasoning_services:
    ChainOfThoughtAgent:
      config:
        max_reasoning_depth: 10
        parallel_chains: 3
        
    CognitiveModelAgent:
      config:
        memory_layers: 8
        attention_heads: 32

# MainPC runs most agent groups
enabled_agent_groups:
  - core_services
  - memory_system  # Only MainPC memory components
  - gpu_infrastructure
  - reasoning_services
  - utility_services
  - learning_services

# MainPC-specific disabled agents from memory_system (PC2 handles these)
disabled_agents:
  - MemoryOrchestratorService  # Runs on PC2

# Hardware-specific tuning
hardware_config:
  gpu:
    type: RTX_4090
    memory_gb: 24
    compute_capability: "8.6"
    max_batch_size: 1024
    
  cpu:
    cores: 16
    threads: 32
    base_frequency: 3.6
    boost_frequency: 5.2
    
  memory:
    total_gb: 32
    available_for_ai: 24
    swap_gb: 16
    
  storage:
    models_path: "/app/models"
    cache_path: "/app/cache"
    logs_path: "/app/logs"
    temp_path: "/tmp/ai_system"

# Docker-specific settings for MainPC
docker:
  compose_file: "docker-compose.mainpc.yml"
  base_image: "nvidia/cuda:12.1-devel-ubuntu22.04"
  runtime: "nvidia"
  
  gpu_support:
    enabled: true
    driver: "nvidia"
    capabilities: ["gpu", "utility", "compute"]
    
  resource_reservations:
    memory: "4096m"
    cpus: "8.0"
    gpu_memory: "20480m"

# Port mappings specific to MainPC (avoid conflicts with PC2)
port_mappings:
  base_offset: 0
  gpu_services_offset: 5000
  reasoning_services_offset: 6000 