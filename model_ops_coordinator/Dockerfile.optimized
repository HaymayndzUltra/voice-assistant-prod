# syntax=docker/dockerfile:1.5
# ModelOpsCoordinator - GPU-heavy LLM service (MainPC/4090)
FROM ghcr.io/haymayndzultra/family-llm-cu121:20250812-latest AS base

ARG MACHINE=mainpc
ENV PYTHONUNBUFFERED=1 \
    TORCH_CUDA_ARCH_LIST="8.9" \
    GPU_VISIBLE_DEVICES=${GPU_VISIBLE_DEVICES:-0}

WORKDIR /app

# Copy machine profile
COPY config/machine-profiles/${MACHINE}.json /etc/machine-profile.json

# Builder stage
FROM base AS builder
COPY requirements/model_ops.txt ./requirements.txt
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install --no-cache-dir --require-hashes -r requirements.txt

# Runtime stage
FROM base AS runtime
COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY model_ops_coordinator/ ./model_ops_coordinator
COPY entrypoints/model_ops_entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

USER appuser

# Health check on port 8212
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -sf http://localhost:8212/health || exit 1

# Expose service and health ports
EXPOSE 7212 8212

ENTRYPOINT ["/usr/bin/tini","--"]
CMD ["/entrypoint.sh"]