#!/usr/bin/env python3
"""
Complete Hybrid AI Setup Test
Local STT (Whisper) + Cloud TTS (OpenAI HD) + Local LLM (Llama3) + Cloud Fallback
"""

import sys
import asyncio
from pathlib import Path

sys.path.append('/home/haymayndz/AI_System_Monorepo')

async def test_complete_setup():
    """Test the complete hybrid AI setup"""
    
    print("ğŸ§ª COMPLETE HYBRID AI SETUP TEST")
    print("=" * 60)
    print("ğŸ“‹ Configuration:")
    print("   ğŸ¤ STT: Local Whisper (primary) â†’ OpenAI Cloud (fallback)")
    print("   ğŸ”Š TTS: OpenAI TTS-1-HD (primary) â†’ ElevenLabs (fallback)")  
    print("   ğŸ§  LLM: Local Llama3 (primary) â†’ OpenAI (fallback)")
    print("   ğŸŒ Translation: Google Cloud (primary) â†’ Azure (fallback)")
    print("=" * 60)
    
    try:
        from common.hybrid_api_manager import api_manager, tts, llm_chat
        
        # Test 1: Local STT (Whisper)
        print("\n1ï¸âƒ£ Testing LOCAL WHISPER STT...")
        try:
            import whisper
            print(f"   âœ… Whisper available: v{whisper.__version__}")
            
            # Create a small test audio (empty for now)
            dummy_audio = b'\x00' * 1024 
            stt_result = await api_manager.speech_to_text(dummy_audio, "en-US")
            print(f"   âœ… STT Result: '{stt_result}'")
            
        except Exception as e:
            print(f"   âŒ Local STT failed: {e}")
            
        # Test 2: Cloud TTS (OpenAI HD)
        print("\n2ï¸âƒ£ Testing CLOUD TTS (OpenAI TTS-1-HD)...")
        try:
            test_text = "Hello! This is a test of OpenAI TTS-1-HD with Nova voice."
            audio_data = await tts(test_text)
            print(f"   âœ… TTS Success: Generated {len(audio_data)} bytes")
            print(f"   Model: {api_manager.config['tts']['primary_provider']} (OpenAI TTS-1-HD)")
            
        except Exception as e:
            print(f"   âŒ Cloud TTS failed: {e}")
            
        # Test 3: Local LLM (Llama3)
        print("\n3ï¸âƒ£ Testing LOCAL LLM (Llama3)...")
        try:
            test_prompt = "What is the capital of the Philippines? Give a brief answer."
            llm_response = await llm_chat(test_prompt, temperature=0.3)
            print(f"   âœ… LLM Response: {llm_response[:100]}...")
            print(f"   Model: {api_manager.config['llm']['primary_provider']} (Llama3)")
            
        except Exception as e:
            print(f"   âŒ Local LLM failed: {e}")
            print("   ğŸ“¥ Llama3 might still be downloading...")
            
        # Test 4: Check Download Status
        print("\n4ï¸âƒ£ Checking Model Status...")
        
        # Check Whisper models
        try:
            import whisper
            available_models = []
            for model in ['tiny', 'base', 'small', 'medium', 'large']:
                try:
                    whisper.load_model(model)
                    available_models.append(model)
                except:
                    pass
            print(f"   ğŸ¤ Whisper models: {', '.join(available_models)}")
        except:
            print("   âŒ Cannot check Whisper models")
            
        # Check Ollama models
        try:
            import subprocess
            result = subprocess.run(['ollama', 'list'], capture_output=True, text=True)
            if result.returncode == 0:
                lines = result.stdout.strip().split('\\n')[1:]  # Skip header
                models = [line.split()[0] for line in lines if line.strip()]
                print(f"   ğŸ§  Ollama models: {', '.join(models)}")
            else:
                print("   âŒ Cannot check Ollama models")
        except:
            print("   âŒ Ollama not available")
            
        print("\n" + "=" * 60)
        print("ğŸ“Š SETUP SUMMARY")
        print("=" * 60)
        
        # Configuration summary
        config = api_manager.config
        
        print("ğŸ¯ Your Optimized Hybrid Configuration:")
        print(f"   STT: {config['stt']['primary_provider']} â†’ {config['stt']['fallback_provider']}")
        print(f"   TTS: {config['tts']['primary_provider']} â†’ {config['tts']['fallback_provider']}")
        print(f"   LLM: {config['llm']['primary_provider']} â†’ {config['llm']['fallback_provider']}")
        print(f"   Translation: {config['translate']['primary_provider']} â†’ {config['translate']['fallback_provider']}")
        
        print("\nğŸ’¡ BENEFITS OF YOUR SETUP:")
        print("   ğŸ¤ STT: Local = Privacy + Speed, No API costs")
        print("   ğŸ”Š TTS: Cloud = High quality, Multiple voices")  
        print("   ğŸ§  LLM: Local = Privacy + Cost savings, Cloud fallback for reliability")
        print("   ğŸŒ Translation: Cloud = Best accuracy")
        
        print("\nğŸš€ Ready for AI Voice Assistant deployment!")
        
    except ImportError as e:
        print(f"âŒ Import Error: {e}")
    except Exception as e:
        print(f"âŒ Test Error: {e}")

if __name__ == "__main__":
    print("ğŸŒŸ HYBRID AI SYSTEM - Complete Setup Test")
    asyncio.run(test_complete_setup())
