"""
Code Generator Agent
- Generates code based on natural language descriptions
- Supports multiple programming languages
- Integrates with the AutoGen framework
- Uses local LLMs for code generation
"""
import os
import uuid
import time
import zmq
import json
import logging
import traceback
import sys
import gc
from typing import Dict, List, Optional, Any, Union, Tuple
from pathlib import Path
import tempfile
import re
import threading

# Add the parent directory to sys.path to import the config module
sys.path.append(str(Path(__file__).parent.parent))
from config.system_config import config

# Import the GGUF Model Manager
from agents.gguf_model_manager import get_instance as get_gguf_manager

# Check for GGUF support
try:
    import llama_cpp
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False
    logger = logging.getLogger("CodeGeneratorAgent")
    logger.warning("llama-cpp-python not installed. GGUF models will not be available.")

# Configure logging
log_level = config.get('system.log_level', 'INFO')
log_file = Path(config.get('system.logs_dir', 'logs')) / "code_generator_agent.log"
log_file.parent.mkdir(exist_ok=True)

logging.basicConfig(
    level=getattr(logging, log_level),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("CodeGeneratorAgent")

# Get ZMQ ports from config
CODE_GENERATOR_PORT = config.get('zmq.code_generator_port', 5604)
MODEL_MANAGER_PORT = config.get('zmq.model_manager_port', 5556)
AUTOGEN_FRAMEWORK_PORT = config.get('zmq.autogen_framework_port', 5600)
EXECUTOR_PORT = config.get('zmq.executor_port', 5613)

class CodeGeneratorAgent:
    """Agent for code generation"""
    def __init__(self):
        logger.info("Initializing CodeGeneratorAgent...")
        
        # Initialize running flag
        self.running = True
        logger.debug(f"Initialized running flag: {self.running}")
        
        # Initialize GGUF model manager if available
        self.gguf_manager = get_gguf_manager() if LLAMA_CPP_AVAILABLE else None
        if self.gguf_manager:
            logger.info("GGUF Model Manager initialized")
        else:
            logger.warning("GGUF Model Manager not available")
            
        # Initialize memory management for GGUF models
        self.memory_check_interval = 60  # Check memory every 60 seconds
        self.loaded_gguf_models = set()  # Track which GGUF models are loaded
        # Add a lock to protect GGUF generation calls (llama.cpp is not thread-safe)
        self._gguf_generation_lock = threading.RLock()
        
        # Initialize ZMQ
        try:
            logger.debug("Creating ZMQ context")
            self.context = zmq.Context()
            logger.debug("ZMQ context created successfully")
            
            # Start memory management thread for GGUF models if available
            if self.gguf_manager:
                self.memory_management_thread = threading.Thread(target=self._gguf_memory_management_loop, daemon=True)
                self.memory_management_thread.start()
                logger.info("Started GGUF memory management thread")
            
            # Socket to receive requests (REP socket for request-reply pattern)
            logger.debug(f"Creating ZMQ REP socket for receiving requests on port {CODE_GENERATOR_PORT}")
            self.receiver = self.context.socket(zmq.REP)
            
            # Set socket options for better debugging and reliability
            self.receiver.setsockopt(zmq.LINGER, 0)  # Don't linger on close
            self.receiver.setsockopt(zmq.RCVTIMEO, 10000)  # 10 second timeout for blocking receives
            logger.debug("Set ZMQ socket options: LINGER=0, RCVTIMEO=10000")
            
            # Bind the socket - use wildcard address to allow remote connections
            logger.debug(f"Attempting to bind REP socket to tcp://*:{CODE_GENERATOR_PORT}")
            self.receiver.bind(f"tcp://*:{CODE_GENERATOR_PORT}")
            logger.info(f"Code Generator Agent successfully bound to port {CODE_GENERATOR_PORT}")
            
            # Socket to communicate with model manager
            logger.debug(f"Creating ZMQ REQ socket for Model Manager on port {MODEL_MANAGER_PORT}")
            self.model_manager = self.context.socket(zmq.REQ)
            self.model_manager.setsockopt(zmq.LINGER, 1000)
            logger.debug(f"Attempting to connect to Model Manager at tcp://localhost:{MODEL_MANAGER_PORT}")
            self.model_manager.connect(f"tcp://localhost:{MODEL_MANAGER_PORT}")
            logger.info(f"Successfully connected to Model Manager on port {MODEL_MANAGER_PORT}")
            
            # Socket to communicate with autogen framework
            logger.debug(f"Creating ZMQ REQ socket for AutoGen Framework on port {AUTOGEN_FRAMEWORK_PORT}")
            self.framework = self.context.socket(zmq.REQ)
            self.framework.setsockopt(zmq.LINGER, 1000)
            # NEW: Set reasonable timeouts so we don't block forever if framework is down
            self.framework.setsockopt(zmq.RCVTIMEO, 5000)  # 5-second receive timeout
            self.framework.setsockopt(zmq.SNDTIMEO, 5000)  # 5-second send timeout
            logger.debug(f"Attempting to connect to AutoGen Framework at tcp://localhost:{AUTOGEN_FRAMEWORK_PORT}")
            self.framework.connect(f"tcp://localhost:{AUTOGEN_FRAMEWORK_PORT}")
            logger.info(f"Successfully connected to AutoGen Framework on port {AUTOGEN_FRAMEWORK_PORT}")
            
            # Socket to communicate with executor agent
            logger.debug(f"Creating ZMQ REQ socket for Executor Agent on port {EXECUTOR_PORT}")
            self.executor = self.context.socket(zmq.REQ)
            self.executor.setsockopt(zmq.LINGER, 1000)
            logger.debug(f"Attempting to connect to Executor Agent at tcp://localhost:{EXECUTOR_PORT}")
            self.executor.connect(f"tcp://localhost:{EXECUTOR_PORT}")
            logger.info(f"Successfully connected to Executor Agent on port {EXECUTOR_PORT}")
            
            logger.info("All ZMQ sockets initialized successfully")
        except zmq.ZMQError as e:
            logger.error(f"ZMQ initialization error: {e}", exc_info=True)
            raise
        except Exception as e:
            logger.error(f"Unexpected error during initialization: {e}", exc_info=True)
            raise
        
        # Setup output directory
        self.output_dir = Path(config.get('system.output_dir', 'output')) / "code"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Model mapping - maps user-friendly names to model IDs in MMA
        # These IDs must match exactly with the ones defined in system_config.py
        self.model_mapping = {
            "phi": "phi",
            "phi3": "phi3",
            "mistral": "mistral",
            "codellama": "codellama-13b",
            # Language to model mappings
            "python": "codellama-13b",  # Best choice for Python
            "javascript": "codellama-13b",  # Good for JS too
            "java": "codellama-13b",  # CodeLlama is good for Java
            "c#": "codellama-13b",  # CodeLlama for C#
            "c++": "codellama-13b",  # CodeLlama for C++
            "html": "codellama-13b"  # CodeLlama can handle HTML
        }
        
        # Debug mode for logging ZMQ messages
        self.debug_mode = config.get('common_settings.system.debug_mode', False)
        
        # Language templates
        self.language_templates = {
            "python": {
                "file_extension": ".py",
                "comment_prefix": "#",
                "imports_section": "# Import necessary libraries\n",
                "main_section": "\n\n# Main function\ndef main():\n    pass\n\n# Run the main function if this script is executed directly\nif __name__ == \"__main__\":\n    main()\n"
            },
            "javascript": {
                "file_extension": ".js",
                "comment_prefix": "//",
                "imports_section": "// Import necessary libraries\n",
                "main_section": "\n\n// Main function\nfunction main() {\n    \n}\n\n// Run the main function\nmain();\n"
            },
            "java": {
                "file_extension": ".java",
                "comment_prefix": "//",
                "imports_section": "// Import necessary libraries\n",
                "main_section": "\n\npublic class Main {\n    public static void main(String[] args) {\n        \n    }\n}\n"
            },
            "c#": {
                "file_extension": ".cs",
                "comment_prefix": "//",
                "imports_section": "// Import necessary libraries\nusing System;\n",
                "main_section": "\n\npublic class Program {\n    public static void Main(string[] args) {\n        \n    }\n}\n"
            },
            "c++": {
                "file_extension": ".cpp",
                "comment_prefix": "//",
                "imports_section": "// Import necessary libraries\n#include <iostream>\n",
                "main_section": "\n\nint main() {\n    \n    return 0;\n}\n"
            },
            "html": {
                "file_extension": ".html",
                "comment_prefix": "<!--",
                "comment_suffix": "-->",
                "imports_section": "",
                "main_section": "<!DOCTYPE html>\n<html>\n<head>\n    <title>Generated Page</title>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n</head>\n<body>\n    \n</body>\n</html>\n"
            }
        }
        
        # Running flag
        self.running = True
        
        logger.info("Code Generator Agent initialized")
    
    def _request_model_access(self, model_id: str, timeout_seconds: int = 60) -> bool:
        """
        Request access to a model from the Model Manager Agent (MMA)
        
        Args:
            model_id (str): The ID of the model to request access to
            timeout_seconds (int): Maximum time to wait for model loading
            
        Returns:
            bool: True if model access was granted, False otherwise
        """
        try:
            # Prepare request for model access - use request_type instead of action for MMA compatibility
            request = {
                "request_type": "load_model",  # MMA uses request_type, not action
                "model_id": model_id,
                "priority": "high",
                "requester": "code_generator_agent",
                "request_id": f"cg_{int(time.time())}"
            }
            
            if self.debug_mode:
                logger.debug(f"Sending MMA request: {json.dumps(request, indent=2)}")
            
            logger.info(f"Requesting access to model {model_id} from MMA")
            self.model_manager.send_string(json.dumps(request))
            
            # Wait for response with timeout
            poller = zmq.Poller()
            poller.register(self.model_manager, zmq.POLLIN)
            
            # Poll for initial response
            if poller.poll(10000):  # 10 second timeout for initial response
                response_str = self.model_manager.recv_string()
                response = json.loads(response_str)
                
                if response.get("status") == "success" and response.get("model_status") == "loaded":
                    logger.info(f"Model {model_id} is already loaded and ready to use")
                    return True
                    
                elif response.get("status") == "pending":
                    logger.info(f"Model {model_id} is being loaded. Waiting for it to be ready...")
                    
                    # Wait for the model to be loaded with polling
                    start_time = time.time()
                    while time.time() - start_time < timeout_seconds:
                        # Check model status - use request_type instead of action for MMA compatibility
                        status_request = {
                            "request_type": "get_model_status",  # MMA uses request_type, not action
                            "model_id": model_id,
                            "request_id": f"cg_status_{int(time.time())}"
                        }
                        
                        if self.debug_mode:
                            logger.debug(f"Sending status check request: {json.dumps(status_request, indent=2)}")
                        
                        self.model_manager.send_string(json.dumps(status_request))
                        
                        if poller.poll(5000):  # 5 second timeout for status check
                            status_response_str = self.model_manager.recv_string()
                            status_response = json.loads(status_response_str)
                            
                            if status_response.get("status") == "success" and status_response.get("model_status") == "loaded":
                                logger.info(f"Model {model_id} is now loaded and ready to use")
                                return True
                        
                        # Wait before polling again
                        time.sleep(2)
                    
                    logger.error(f"Timeout waiting for model {model_id} to load")
                    return False
                else:
                    logger.error(f"Error requesting model access: {response.get('error', 'Unknown error')}")
                    return False
            else:
                logger.error("Timeout waiting for initial response from MMA")
                return False
                
        except Exception as e:
            logger.error(f"Error requesting model access: {str(e)}")
            return False
    
    def _send_to_llm(self, prompt: str, system_prompt: Optional[str] = None, model: str = "codellama") -> str:
        """Send a prompt to the LLM through the model manager or directly via GGUF if available"""
        logger.debug(f"Sending prompt to LLM, model: {model}")

        # Map the model name to the configured model_id
        model_id = self.model_mapping.get(model, model)  # Default to provided name
        logger.debug(f"Mapped model '{model}' to model_id '{model_id}'")

        # --- GGUF DIRECT PATH --------------------------------------------------
        if self.gguf_manager and model_id in self.gguf_manager.model_metadata:
            if not LLAMA_CPP_AVAILABLE:
                logger.error("llama-cpp-python not installed – cannot serve GGUF models")
                raise RuntimeError("llama-cpp-python library missing")
            try:
                # On-demand load
                if self.gguf_manager.load_model(model_id):
                    self.loaded_gguf_models.add(model_id)
                else:
                    raise RuntimeError(f"Unable to load GGUF model {model_id}")

                # Generate using llama.cpp – ensure single threaded access
                with self._gguf_generation_lock:
                    llama_model = self.gguf_manager.loaded_models[model_id]
                    combined_prompt = f"{system_prompt}\n\n{prompt}" if system_prompt else prompt
                    logger.debug(f"Calling llama.cpp for model {model_id}")
                    response = llama_model(
                        combined_prompt,
                        max_tokens=512,
                        temperature=0.1,
                        stop=["```", "<|end|>"]
                    )
                text = response["choices"][0]["text"].strip()
                logger.info(f"GGUF generation successful using model {model_id}")
                return text
            except Exception as e:
                logger.error(f"Error during GGUF generation with {model_id}: {e}")
                raise

        # --- DEFAULT MMA PATH --------------------------------------------------
        try:
            request = {
                "request_type": "generate",
                "model_id": model_id,
                "prompt": prompt,
                "request_id": f"cg_{int(time.time())}"
            }
            if system_prompt:
                request["system_prompt"] = system_prompt
            logger.debug(f"Prepared LLM request for model {model_id} with request_id {request['request_id']}")
            self.model_manager.send_json(request)
            poller = zmq.Poller()
            poller.register(self.model_manager, zmq.POLLIN)
            if poller.poll(30000):  # 30 s timeout
                response = self.model_manager.recv_json()
                if response.get("status") == "success":
                    logger.info(f"Successfully received response from model {model_id}")
                    return response.get("text", "")
                else:
                    logger.error(f"Error from model manager: {response.get('error', 'Unknown error')}")
                    raise Exception(response.get("error", "Unknown error"))
            else:
                logger.error("Timeout waiting for response from model manager")
                raise Exception("Timeout waiting for response from model manager")
        except Exception as e:
            logger.error(f"Error sending to LLM via MMA: {str(e)}")
            raise
    
    def _map_model_name_to_id(self, model_name: str) -> str:
        """
        Map a user-friendly model name to its corresponding model ID in the MMA
        
        Args:
            model_name (str): User-friendly model name or language
            
        Returns:
            str: The corresponding model ID for the MMA
        """
        # Convert to lowercase for case-insensitive matching
        model_name_lower = model_name.lower()
        
        # First check if we have a direct mapping
        if model_name_lower in self.model_mapping:
            return self.model_mapping[model_name_lower]
        
        # If not found, default to codellama-13b which is available in our system
        logger.info(f"No specific mapping found for model '{model_name}', using default model 'codellama-13b'")
        return "codellama-13b"
    
    def _execute_code(self, code: str, parameters: Dict[str, Any] = None) -> Dict[str, Any]:
        """Execute code using the executor agent"""
        try:
            # Prepare request
            request = {
                "request_type": "execute",
                "code": code,
                "parameters": parameters or {}
            }
            
            # Send request to executor agent
            self.executor.send_json(request)
            
            # Wait for response with timeout
            poller = zmq.Poller()
            poller.register(self.executor, zmq.POLLIN)
            
            if poller.poll(30000):  # 30 second timeout
                response = self.executor.recv_json()
                
                if response["status"] == "success":
                    return response["result"]
                else:
                    logger.error(f"Error from executor agent: {response.get('error', 'Unknown error')}")
                    raise Exception(response.get("error", "Unknown error"))
            else:
                logger.error("Timeout waiting for response from executor agent")
                raise Exception("Timeout waiting for response from executor agent")
        
        except Exception as e:
            logger.error(f"Error executing code: {str(e)}")
            raise
    
    def _save_code_to_file(self, code: str, language: str, filename: Optional[str] = None) -> str:
        """Save code to a file"""
        # Get file extension for the language
        file_extension = self.language_templates.get(language.lower(), {}).get("file_extension", ".txt")
        
        # Generate filename if not provided
        if not filename:
            timestamp = int(time.time())
            filename = f"generated_code_{timestamp}{file_extension}"
        elif not filename.endswith(file_extension):
            filename = f"{filename}{file_extension}"
        
        # Save code to file
        file_path = self.output_dir / filename
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(code)
        
        logger.info(f"Saved code to {file_path}")
        
        return str(file_path)
    
    def _detect_language_from_description(self, description: str) -> str:
        """Detect programming language from description"""
        description_lower = description.lower()
        
        # Check for explicit language mentions
        if "python" in description_lower:
            return "python"
        elif "javascript" in description_lower or "js" in description_lower:
            return "javascript"
        elif "java" in description_lower and "javascript" not in description_lower:
            return "java"
        elif "c#" in description_lower or "csharp" in description_lower:
            return "c#"
        elif "c++" in description_lower or "cpp" in description_lower:
            return "c++"
        elif "html" in description_lower:
            return "html"
        
        # Default to Python if no language is specified
        return "python"
    
    def _extract_code_from_llm_response(self, response: str) -> str:
        """Extract code from LLM response"""
        # Try to extract code from markdown code blocks
        code_block_pattern = r"```(?:\w+)?\s*([\s\S]*?)```"
        code_blocks = re.findall(code_block_pattern, response)
        
        if code_blocks:
            # Join all code blocks
            return "\n\n".join(code_blocks)
        else:
            # If no code blocks found, return the entire response
            return response
            
    def _vote_on_responses(self, responses: List[str]) -> str:
        """
        Implement a voting mechanism to select the best code from multiple model responses
        
        Args:
            responses (List[str]): List of code responses from different models
            
        Returns:
            str: The selected 'best' code
        """
        if not responses:
            return ""
            
        if len(responses) == 1:
            return responses[0]
            
        # Implementation of a similarity-based voting mechanism
        # We'll use a simple approach: find the response that is most similar to others
        # This assumes that correct solutions will be more similar to each other
        
        # Calculate similarity scores for each response
        similarity_scores = [0] * len(responses)
        
        for i in range(len(responses)):
            for j in range(len(responses)):
                if i != j:
                    # Simple similarity measure: count common lines
                    lines_i = set(responses[i].strip().split('\n'))
                    lines_j = set(responses[j].strip().split('\n'))
                    common_lines = len(lines_i.intersection(lines_j))
                    
                    # Weight by the fraction of common lines
                    if lines_i and lines_j:  # Avoid division by zero
                        similarity = common_lines / max(len(lines_i), len(lines_j))
                        similarity_scores[i] += similarity
        
        # Find the response with the highest similarity score
        best_index = similarity_scores.index(max(similarity_scores))
        logger.info(f"Selected response {best_index+1}/{len(responses)} as the best based on voting")
        
        return responses[best_index]

    def generate_code(self, description: str, language: Optional[str] = None, save_to_file: bool = True, use_voting: bool = False) -> Dict[str, Any]:
        """Generate code based on description, optionally using model voting"""
        logger.info("Generating code from description" + (" with voting" if use_voting else ""))
        if use_voting:
            return self.generate_with_voting(description, language, save_to_file)
        if not language:
            language = self._detect_language_from_description(description)
            
        # Create a system prompt based on the language
        system_prompt = f"You are an expert {language} developer. Generate clean, efficient, and well-documented {language} code."
        
        prompt = f"""Generate {language or 'code'} for the following task:\n{description}"""
        response = self._send_to_llm(prompt, system_prompt=system_prompt, model=language or "deepseek")
        code = self._extract_code_from_llm_response(response)
        if save_to_file:
            filename = self._save_code_to_file(code, language)
        else:
            filename = None
        return {
            "code": code,
            "language": language,
            "filename": filename
        }

    def fix_code(self, code: str, error: str, language: Optional[str] = None, use_voting: bool = False) -> Dict[str, Any]:
        """Fix code based on error feedback, optionally using model voting"""
        logger.info("Fixing code based on error feedback" + (" with voting" if use_voting else ""))
        if use_voting:
            prompt = (
                f"The following {language or 'code'} has an error.\n"
                f"Error message: {error}\n"
                f"Please fix the code below so it works correctly.\n"
                f"---\n"
                f"{code}\n"
                f"---\n"
                f"Return only the fixed code."
            )
            # Use voting across models for fixing
            voting_result = self.generate_with_voting(prompt, language, save_to_file=False)
            # Add fixed_code field for backward compatibility
            voting_result["fixed_code"] = voting_result["code"]
            return voting_result
        if not language:
            language = self._detect_language_from_description(code)
        prompt = (
            f"The following {language or 'code'} has an error.\n"
            f"Error message: {error}\n"
            f"Please fix the code below so it works correctly.\n"
            f"---\n"
            f"{code}\n"
            f"---\n"
            f"Return only the fixed code."
        )
        response = self._send_to_llm(prompt, model=language or "deepseek")
        fixed_code = self._extract_code_from_llm_response(response)
        return {
            "fixed_code": fixed_code,
            "language": language
        }

    def generate_with_voting(self, description: str, language: Optional[str] = None, save_to_file: bool = True) -> Dict[str, Any]:
        """Generate code using multiple models and voting"""
        if not language:
            language = self._detect_language_from_description(description)
            
        # Create a system prompt based on the language
        system_prompt = f"You are an expert {language} developer. Generate clean, efficient, and well-documented {language} code."
            
        # Use multiple models for voting - map to model IDs
        models = ["codellama-13b", "phi3", "mistral"]
        responses = []
        
        for model_id in models:
            try:
                # Request model access first
                if not self._request_model_access(model_id):
                    logger.warning(f"Failed to get access to model {model_id}, skipping it for voting")
                    continue
                    
                logger.info(f"Generating code with model: {model_id}")
                prompt = f"""Generate {language or 'code'} for the following task:\n{description}"""
                
                # Send generation request directly with model_id
                request = {
                    "request_type": "generate",  # MMA uses request_type, not action
                    "model_id": model_id,
                    "prompt": prompt,
                    "system_prompt": system_prompt,
                    "temperature": 0.2,
                    "request_id": f"cg_voting_{int(time.time())}"
                }
                
                if self.debug_mode:
                    logger.debug(f"Sending voting generation request: {json.dumps(request, indent=2)}")
                
                self.model_manager.send_json(request)
                
                # Wait for response with timeout
                poller = zmq.Poller()
                poller.register(self.model_manager, zmq.POLLIN)
                
                if poller.poll(30000):  # 30 second timeout
                    response = self.model_manager.recv_json()
                    
                    if response["status"] == "success":
                        code = self._extract_code_from_llm_response(response["text"])
                        responses.append(code)
                        logger.info(f"Successfully generated code with {model_id}")
                    else:
                        logger.error(f"Error from model {model_id}: {response.get('error', 'Unknown error')}")
                else:
                    logger.error(f"Timeout waiting for response from model {model_id}")
            except Exception as e:
                logger.error(f"Error generating code with {model_id}: {str(e)}")
                # Continue with other models if one fails
        
        # Only proceed with voting if we have at least 2 responses
        if len(responses) < 2:
            if responses:  # If we have at least one response
                code = responses[0]
            else:  # Fallback if all models failed
                logger.error("All models failed, returning error")
                return {"error": "All models failed to generate code"}
        else:
            # Simple voting: find the most similar responses
            # This is a very basic implementation and could be improved
            code = self._vote_on_responses(responses)
            
        # Save to file if requested
        filename = self._save_code_to_file(code, language) if save_to_file else None
        
        return {
            "code": code,
            "language": language,
            "filename": filename
        }

    def run(self):
        """Main entry point for the agent"""
        logger.info("Starting Code Generator Agent run method")
        print(f"[CGA] Starting Code Generator Agent on port {CODE_GENERATOR_PORT}")
        sys.stdout.flush()
        
        # Debug modes for easier testing
        if '--simple-debug' in sys.argv:
            print("[CGA] Running in simple debug mode - only handling ping requests")
            sys.stdout.flush()
            self._simple_debug_loop()
            return
        elif '--debug-mode' in sys.argv:
            print("[CGA] Running in debug mode - independent operation without AutoGen Framework")
            sys.stdout.flush()
            # Skip AutoGen Framework registration
            self.skip_autogen_registration = True
        else:
            self.skip_autogen_registration = False
        
        # Try to register with AutoGen Framework with timeout
        if hasattr(self, 'framework') and not self.skip_autogen_registration:
            try:
                logger.info("Attempting to register with AutoGen Framework...")
                print("[CGA] Attempting to register with AutoGen Framework (2 sec timeout)...")
                sys.stdout.flush()
                
                # Prepare registration request
                registration_request = {
                    "request_type": "register",
                    "agent_type": "code_generator",
                    "port": CODE_GENERATOR_PORT
                }
                
                # Send registration request
                self.framework.send_json(registration_request)
                
                # Set up poller with timeout
                poller = zmq.Poller()
                poller.register(self.framework, zmq.POLLIN)
                
                if poller.poll(2000):  # 2-second timeout
                    # Successfully received response within timeout
                    response = self.framework.recv_json()
                    logger.info(f"AutoGen Framework registration response: {response}")
                    print(f"[CGA] AutoGen Framework registration response: {response}")
                    sys.stdout.flush()
                else:
                    # Timeout waiting for response
                    logger.warning("Timeout registering with AutoGen Framework. Proceeding anyway...")
                    print("[CGA] Timeout registering with AutoGen Framework. Proceeding anyway...")
                    sys.stdout.flush()
            except Exception as e_autogen:
                # Error during registration attempt
                logger.warning(f"Could not register with AutoGen Framework: {str(e_autogen)}. Proceeding anyway...")
                print(f"[CGA] Could not register with AutoGen Framework: {str(e_autogen)}. Proceeding anyway...")
                sys.stdout.flush()
        else:
            logger.info("No AutoGen Framework socket available, skipping registration")
            print("[CGA] No AutoGen Framework socket available, skipping registration")
            sys.stdout.flush()
        
        # Proceed to handle requests regardless of registration status
        print("[CGA] Proceeding to handle_requests() loop")
        sys.stdout.flush()
        try:
            self.handle_requests()
        except KeyboardInterrupt:
            logger.info("Received keyboard interrupt, shutting down")
            print("[CGA] Received keyboard interrupt, shutting down")
            sys.stdout.flush()
        except Exception as e:
            logger.error(f"Error in handle_requests(): {e}", exc_info=True)
            print(f"[CGA ERROR] handle_requests() failed: {e}")
            sys.stdout.flush()
            traceback.print_exc()
        finally:
            # Clean shutdown
            logger.info("Closing ZMQ socket and context")
            print("[CGA] Shutting down...")
            sys.stdout.flush()
            if hasattr(self, 'receiver'):
                self.receiver.close()
            if hasattr(self, 'context'):
                self.context.term()
            print("[CGA] Shutdown complete")
            sys.stdout.flush()
    
    def _simple_debug_loop(self):
        """Very simple message loop for debugging - only handles ping requests"""
        print("[CGA] Starting simple debug message loop")
        sys.stdout.flush()
        
        while True:
            print("[CGA] Waiting for request in simple debug loop...")
            sys.stdout.flush()
            try:
                # Set explicit timeout
                self.receiver.setsockopt(zmq.RCVTIMEO, 10000)  # 10 second timeout
                
                # Wait for request
                request = self.receiver.recv_json()
                print(f"[CGA] Received request: {request}")
                sys.stdout.flush()
                
                # Send simple pong response
                response = {
                    "status": "success",
                    "message": "pong", 
                    "timestamp": time.time(),
                    "request_id": request.get("request_id", "unknown")
                }
                
                print(f"[CGA] Sending response: {json.dumps(response, indent=2)}")
                sys.stdout.flush()
                
                # Use send_string with json.dumps for maximum compatibility with MMA
                response_str = json.dumps(response)
                self.receiver.send_string(response_str)
                
                print(f"[CGA] Response sent successfully as string")
                sys.stdout.flush()
                logger.debug("Response sent successfully")
                
            except zmq.error.Again:
                # Timeout, just continue
                print("[CGA] Socket receive timeout - continuing")
                sys.stdout.flush()
            except Exception as e:
                print(f"[CGA] Error in simple debug loop: {e}")
                sys.stdout.flush()
                traceback.print_exc()
        
    def _gguf_memory_management_loop(self):
        """Background thread for monitoring GGUF model memory usage"""
        logger.info("Started GGUF memory management loop")
        while self.running:
            try:
                if self.gguf_manager:
                    # Check for idle models
                    unloaded = self.gguf_manager.check_idle_models()
                    if unloaded:
                        logger.info(f"Unloaded idle GGUF models: {unloaded}")
                        for model_id in unloaded:
                            if model_id in self.loaded_gguf_models:
                                self.loaded_gguf_models.remove(model_id)
            except Exception as e:
                logger.error(f"Error in GGUF memory management: {e}")
                traceback.print_exc()
            
            # Sleep for the check interval
            for _ in range(self.memory_check_interval):
                if not self.running:
                    break
                time.sleep(1)
    
    def load_gguf_model(self, model_id: str) -> Dict[str, Any]:
        """Load a GGUF model
        
        Args:
            model_id: ID of the model to load
            
        Returns:
            Dict with status and error message if any
        """
        if not self.gguf_manager:
            return {"status": "error", "error": "GGUF Model Manager not available"}
        
        try:
            logger.info(f"Loading GGUF model: {model_id}")
            success = self.gguf_manager.load_model(model_id)
            
            if success:
                self.loaded_gguf_models.add(model_id)
                return {"status": "success", "message": f"Model {model_id} loaded successfully"}
            else:
                return {"status": "error", "error": f"Failed to load model {model_id}"}
                
        except Exception as e:
            logger.error(f"Error loading GGUF model {model_id}: {e}")
            return {"status": "error", "error": str(e)}
    
    def unload_gguf_model(self, model_id: str) -> Dict[str, Any]:
        """Unload a GGUF model
        
        Args:
            model_id: ID of the model to unload
            
        Returns:
            Dict with status and error message if any
        """
        if not self.gguf_manager:
            return {"status": "error", "error": "GGUF Model Manager not available"}
        
        try:
            logger.info(f"Unloading GGUF model: {model_id}")
            success = self.gguf_manager.unload_model(model_id)
            
            if success:
                if model_id in self.loaded_gguf_models:
                    self.loaded_gguf_models.remove(model_id)
                return {"status": "success", "message": f"Model {model_id} unloaded successfully"}
            else:
                return {"status": "error", "error": f"Failed to unload model {model_id}"}
                
        except Exception as e:
            logger.error(f"Error unloading GGUF model {model_id}: {e}")
            return {"status": "error", "error": str(e)}
    
    def generate_with_gguf(self, model_id: str, prompt: str, system_prompt: Optional[str] = None, 
            max_tokens: int = 1024, temperature: float = 0.7) -> Dict[str, Any]:
        """Generate text using a GGUF model
        
        Args:
            model_id: ID of the model to use
            prompt: The prompt to generate text from
            system_prompt: Optional system prompt
            max_tokens: Maximum tokens to generate
            temperature: Temperature for sampling
            
        Returns:
            Dict with generated text and metadata
        """
        if not self.gguf_manager:
            return {"status": "error", "error": "GGUF Model Manager not available"}
        
        try:
            logger.info(f"Generating text with GGUF model {model_id}")
            
            # Use the GGUF manager to generate text
            result = self.gguf_manager.generate_text(
                model_id=model_id,
                prompt=prompt,
                system_prompt=system_prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                stop=["```", "</code>"],
            )
            
            if "error" in result:
                return {"status": "error", "error": result["error"]}
            
            code = self._extract_code_from_llm_response(result["text"])
            
            return {
                "status": "success",
                "generated_code": code,
                "model_used": model_id,
                "full_response": result["text"]
            }
                
        except Exception as e:
            logger.error(f"Error generating with GGUF model {model_id}: {e}")
            return {"status": "error", "error": str(e)}
    
    def handle_requests(self):
        """Handle incoming requests"""
        logger.debug("Entering handle_requests loop")
        
        while self.running:
            try:
                # Wait for a request with timeout
                if self.receiver.poll(1000) == 0:  # 1 second timeout
                    continue
                
                # Get the request
                request_str = self.receiver.recv_string()
                logger.debug(f"Received request: {request_str}")
                sys.stdout.flush()
                
                # Send simple pong response
                response = {
                    "status": "success",
                    "message": "pong", 
                    "timestamp": time.time(),
                    "request_id": "unknown",
                    "request_timestamp": 0,
                    "processing_time_ms": 0
                }
                
                try:
                    request = json.loads(request_str)
                    action = request.get('action', '')
                    
                    if action == 'ping':
                        response = {"status": "ok", "message": "CodeGeneratorAgent is running", "timestamp": time.time()}
                        
                    elif action == 'load_gguf_model':
                        model_id = request.get('model_id')
                        if not model_id:
                            response = {"status": "error", "error": "Missing model_id parameter"}
                        else:
                            response = self.load_gguf_model(model_id)
                            
                    elif action == 'unload_gguf_model':
                        model_id = request.get('model_id')
                        if not model_id:
                            
                elif action == 'generate_with_gguf':
                    model_id = request.get('model_id')
                    prompt = request.get('prompt')
                    system_prompt = request.get('system_prompt')
                    max_tokens = request.get('max_tokens', 1024)
                    temperature = request.get('temperature', 0.7)
                        
                    if not model_id or not prompt:
                        response = {"status": "error", "error": "Missing required parameters"}
                    else:
                        response = self.generate_with_gguf(
                            model_id=model_id,
                            prompt=prompt,
                            system_prompt=system_prompt,
                            max_tokens=max_tokens,
                            temperature=temperature
                        )
                        
                elif action == 'get_gguf_status':
                    # Get status of all GGUF models
                    if self.gguf_manager:
                        try:
                            models = self.gguf_manager.list_models()
                            status = {}
                            for model in models:
                                model_id = model.get('model_id')
                                status[model_id] = model.get('status', 'unknown')
                                
                            response = {
                                "status": "success", 
                                "models": status,
                                "timestamp": time.time()
                            }
                        except Exception as e:
                            logger.error(f"Error getting GGUF model status: {e}")
                            response = {"status": "error", "error": str(e)}
                    else:
                        response = {
                            "status": "error", 
                            "error": "GGUF manager not available",
                            "timestamp": time.time()
                        }
                        
                elif action == 'list_gguf_models':
                    # List all available GGUF models
                    if self.gguf_manager:
                        try:
                            models = self.gguf_manager.list_models()
                            response = {
                                "status": "success", 
                                "models": models,
                                "timestamp": time.time()
                            }
                        except Exception as e:
                            logger.error(f"Error listing GGUF models: {e}")
                            response = {"status": "error", "error": str(e)}
                    else:
                        response = {
                            "status": "error", 
                            "error": "GGUF manager not available",
                            "timestamp": time.time()
                        }

                elif action == 'generate_code':
                    # Generate code based on the description
                    description = request.get('description', '')
                    language = request.get('language')
                    use_voting = request.get("use_voting", False)
                    save_to_file = request.get("save_to_file", True)
                    model_id = request.get('model_id')  # Optional specific GGUF model to load
                    request_id = request.get('request_id', f"gen_{int(time.time())}")
                        
                    logger.info(f"Code generation request (ID: {request_id}): desc len={len(description)}, lang={language}, model_id={model_id}")
                        
                    if not description:
                        response = {
                            'status': 'error',
                            'error': 'Missing description parameter',
                            'request_id': request_id
                        }
                    else:
                        try:
                            start_time = time.time()
                            # If a specific GGUF model is requested, attempt to load it
                            if model_id:
                                load_res = self.load_gguf_model(model_id)
                                if load_res.get('status') != 'success':
                                    logger.warning(f"Could not load GGUF model {model_id}: {load_res.get('error')}")
                            
                            gen_res = self.generate_code(
                                description=description,
                                language=language,
                                save_to_file=save_to_file,
                                use_voting=use_voting
                            )
                            
                        use_voting = request.get("use_voting", False)
                        save_to_file = request.get("save_to_file", True)
                        model_id = request.get('model_id')  # Optional specific GGUF model to load
                        request_id = request.get('request_id', f"gen_{int(time.time())}")
                        
                        logger.info(f"Code generation request (ID: {request_id}): desc len={len(description)}, lang={language}, model_id={model_id}")
                        
                        if not description:
                            response = {
                                'status': 'error',
                                'error': 'Missing description parameter',
                                'request_id': request_id
                            }
                        else:
                            try:
                                start_time = time.time()
                                # If a specific GGUF model is requested, attempt to load it
                                if model_id:
                                    load_res = self.load_gguf_model(model_id)
                                    if load_res.get('status') != 'success':
                                        logger.warning(f"Could not load GGUF model {model_id}: {load_res.get('error')}")
                                
                                gen_res = self.generate_code(
                                    description=description,
                                    language=language,
                                    save_to_file=save_to_file,
                                    use_voting=use_voting
                                )
                                
                                response = {
                                    'status': 'success',
                                    'generated_code': gen_res.get('code', ''),
                                    'language': gen_res.get('language', language),
                                    'filename': gen_res.get('filename'),
                                    'processing_time_ms': int((time.time() - start_time) * 1000),
                                    'request_id': request_id
                                }
                            except Exception as e:
                                logger.error(f"Error in generate_code for request_id '{request_id}': {str(e)}", exc_info=True)
                                response = {
                                    'status': 'error',
                                    'error': f"CGA Error: {str(e)}",
                                    'request_id': request_id,
                                    'model_tried': model_id if model_id else language or 'default',
                                    'trace': traceback.format_exc()
                                }
                            finally:
                                if model_id:
                                    self.unload_gguf_model(model_id)
                    else:
                        logger.warning(f"Unknown request type: {request.get('request_type')} or action: {request.get('action')}")
                        response = {
                            "status": "error",
                            "error": f"Unknown request type: {request.get('request_type')} or action: {request.get('action')}"
                        }
                except KeyError as ke:
                    logger.error(f"Missing required field in request: {ke}")
                    response = {
                        "status": "error",
                        "error": f"Missing required field: {str(ke)}"
                    }
                except Exception as e:
                    logger.error(f"Error processing request: {e}", exc_info=True)
                    response = {
                        "status": "error",
                        "error": f"Error processing request: {str(e)}"
                    }
                
                # Send response
                logger.debug(f"Sending response: {str(response)[:200]}..." if len(str(response)) > 200 else str(response))
                print(f"[CGA] Sending response: {json.dumps(response)[:200]}..." if len(json.dumps(response)) > 200 else f"[CGA] Sending response: {json.dumps(response)}")
                sys.stdout.flush()
                
                # Use send_string with json.dumps for maximum compatibility with MMA
                response_str = json.dumps(response)
                self.receiver.send_string(response_str)
                
                logger.info(f"Successfully sent response for request ID: {request.get('request_id', 'no-id')}")
                print(f"[CGA] Successfully sent response as string for request ID: {request.get('request_id', 'no-id')}")
                sys.stdout.flush()
            
            except zmq.Again:
                # Timeout, continue loop
                logger.debug("ZMQ socket timeout (zmq.Again), continuing loop")
                print("[CGA] ZMQ socket timeout, continuing loop")
                sys.stdout.flush()
            except Exception as e:
                logger.error(f"Unhandled error in request handler: {e}", exc_info=True)
                traceback.print_exc()
                
                # Try to recover by sending an error response if possible
                try:
                    error_response = {
                        "status": "error",
                        "error": f"Unhandled server error: {str(e)}"
                    }
                    self.receiver.send_string(json.dumps(error_response))
                    logger.info("Sent error response for unhandled exception")
                except:
                    logger.error("Could not send error response for unhandled exception")
        
        logger.info("Message handling loop terminated")
    
    def run(self):
        """Run the code generator agent"""
        try:
            # Register with AutoGen framework
            try:
                self.framework.send_string(json.dumps({
                    "request_type": "register_agent",
                    "agent_id": "code_generator",
                    "endpoint": f"tcp://localhost:{CODE_GENERATOR_PORT}",
                    "capabilities": ["code_generation"]
                }))
            except zmq.error.Again:
                logger.warning("AutoGen framework not reachable (send timeout). Proceeding without registration.")
            
            # Wait for response (but don't block forever)
            try:
                response_str = self.framework.recv_string()
                response = json.loads(response_str)
                if response.get("status") == "success":
                    logger.info("Registered with AutoGen framework")
                else:
                    logger.warning(f"AutoGen framework replied with error: {response.get('error', 'unknown')}. Continuing anyway.")
            except zmq.error.Again:
                logger.warning("No response from AutoGen framework within timeout – continuing without registration")
            except Exception as e:
                logger.warning(f"Unexpected error talking to AutoGen framework: {e}. Continuing anyway.")
            
            # Main request handling loop
            self.handle_requests()
                
        except KeyboardInterrupt:
            logger.info("Interrupted by user")
        except Exception as e:
            logger.error(f"Error in main loop: {str(e)}")
            traceback.print_exc()
        finally:
            # Clean shutdown
            logger.info("Closing ZMQ socket and context")
            print("[CGA] Shutting down...")
            sys.stdout.flush()
            if hasattr(self, 'receiver'):
                self.receiver.close()
            if hasattr(self, 'context'):
                self.context.term()
            print("[CGA] Shutdown complete")
            sys.stdout.flush()
    
    def cleanup(self):
        """Clean up resources"""
        self.running = False
        
        # Unregister from AutoGen framework
        try:
            self.framework.send_string(json.dumps({
                "request_type": "unregister_agent",
                "agent_id": "code_generator"
            }))
            
            # Wait for response
            response_str = self.framework.recv_string()
        except:
            pass
        
        # Clean up GGUF models if available
        if self.gguf_manager:
            logger.info("Cleaning up GGUF models...")
            self.gguf_manager.cleanup()
        
        self.receiver.close()
        self.model_manager.close()
        self.framework.close()
        self.executor.close()
        self.context.term()
        
        logger.info("Code Generator Agent stopped")
        code = self._extract_code_from_llm_response(response)
        if save_to_file:
            filename = self._save_code_to_file(code, language)
        else:
            filename = None
        return {
            "code": code,
            "language": language,
            "filename": filename
        }

    def fix_code(self, code: str, error: str, language: Optional[str] = None, use_voting: bool = False) -> Dict[str, Any]:
        """Fix code based on error feedback, optionally using model voting"""
        logger.info("Fixing code based on error feedback" + (" with voting" if use_voting else ""))
        if use_voting:
            prompt = (
                f"The following {language or 'code'} has an error.\n"
                f"Error message: {error}\n"
                f"Please fix the code below so it works correctly.\n"
                f"---\n"
                f"{code}\n"
                f"---\n"
                f"Return only the fixed code."
            )
            # Use voting across models for fixing
            voting_result = self.generate_with_voting(prompt, language, save_to_file=False)
            # Add fixed_code field for backward compatibility
            voting_result["fixed_code"] = voting_result["code"]
            return voting_result
        if not language:
            language = self._detect_language_from_description(code)
        prompt = (
            f"The following {language or 'code'} has an error.\n"
            f"Error message: {error}\n"
            f"Please fix the code below so it works correctly.\n"
            f"---\n"
            f"{code}\n"
            f"---\n"
            f"Return only the fixed code."
        )
        response = self._send_to_llm(prompt, model=language or "deepseek")
        fixed_code = self._extract_code_from_llm_response(response)
        return {
            "fixed_code": fixed_code,
            "language": language
        }

# Main entry point
if __name__ == "__main__":

    try:
        logger.info("Starting Code Generator Agent...")
        agent = CodeGeneratorAgent()
        agent.run()
    except KeyboardInterrupt:
        logger.info("Code Generator Agent interrupted by user")
    except Exception as e:
        logger.error(f"Error running Code Generator Agent: {str(e)}")
        traceback.print_exc()
